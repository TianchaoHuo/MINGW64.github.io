<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Change-detection-using-GMM</title>
      <link href="/2019/07/12/Change-detection-using-GMM/"/>
      <url>/2019/07/12/Change-detection-using-GMM/</url>
      
        <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Change detection is one of the most commonly used methods for detecting moving objects in a computer vision system. It detects moving objects in an image sequence by comparing the background model with the current frame. Traditional moving target detection algorithms have problems such as dynamic background changes caused by illumination changes, noise and shadow sensitivity.</p><a id="more"></a><p>In this project, a method based on Gaussian Mixture Model (GMM) is introduced for the challenge of object detection, which modeling each pixel of each frame as a mixture of Gaussians and using an on-line approximation to update the model. Based on whether the Gaussian distribution that represents it most effectively is considered part of the background model, each pixel is classified. This report is targeted to utilizes the python to implement the GMM algorithm to segment background for several videos<br><br><br>Repo: <a href="https://github.com/TianchaoHuo/Change-Detection-Using-GMM" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Change-Detection-Using-GMM</a></p><h2 id="Objective"><a href="#Objective" class="headerlink" title="Objective:"></a>Objective:</h2><p>1.Detecting the moving object in videos recorded by a stationary camera<br>2.Segmentation between foreground and background<br>3.Adaptive in working conditions<br>4.Not costly develop</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>PETS 2009 (<a href="ftp://pets.rdg.ac.uk" target="_blank" rel="noopener">ftp://pets.rdg.ac.uk</a>)</p><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><p>This implementataion is based on the python 3 and open-cv package.<br>Hence, you need to install the related implementation environment.</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=58925991&cid=102725017&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>Pros</li></ul><ol><li>Be adaptive to different scenes</li><li>Be capable to segment videos</li><li>Good trade-off between quality and execution time</li></ol><ul><li>Cons</li></ul><ol><li>Arbitrarily set the parameters</li><li>Assume each pixel is independent of its neighbors.</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] C. Stauffer and W. E. L. Grimson, “Adaptive background mixture models for real-time tracking,” in 1999, . DOI: 10.1109/CVPR.1999.784637.<br>[2] H. Shih-Shinh, <a href="https://www.youtube.com/watch?v=g_ve2txPkSc" target="_blank" rel="noopener">https://www.youtube.com/watch?v=g_ve2txPkSc</a>, 2018.<br>[3] Swapnil Das, <a href="https://github.com/swapnil96" target="_blank" rel="noopener">https://github.com/swapnil96</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Friut Detection, Tracking and Counting</title>
      <link href="/2019/07/11/FriutDetectionTrackingandCounting/"/>
      <url>/2019/07/11/FriutDetectionTrackingandCounting/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><hr><p>This project is designed to do fruit detection, traking and counting. Videos are already provided.</p><p>Co-Author: Chaohuo Wu, Xiaochen Liu, Do Kyoun Lee</p><p>Repo: <a href="https://github.com/TianchaoHuo/Fruit-Detection" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Fruit-Detection</a></p><a id="more"></a><h1 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h1><hr><p> 1 . Read in a video clip, and display the video frame by frame,<br> 2 . Detect all lemons and bananas in each frame, segment the corresponding pixel regions, count the number of pixels, draw a bounding box on these detected fruits.<br> 3 . Track the detected oranges and bananas in the entire video sequence, until they disappear.<br> 4 . Count the number of fruits of each type, and update the counting result in real-time.<br> 5 . At the end of each video clip, report the success rate of your program.</p><h1 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h1><hr><p>Open the video named ‘Test.mp4’. This is the one of the testing video.  </p><h1 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h1><hr><p>1 . DownLoad the project material from google driver. The link is: <a href="https://drive.google.com/open?id=1yQXSwYu-GljAxXZLB0YlPoV0lx7KVMPh" target="_blank" rel="noopener">https://drive.google.com/open?id=1yQXSwYu-GljAxXZLB0YlPoV0lx7KVMPh</a><br>2 . Open MATLAB software.<br>3 . Decompress the ‘project material.zip’ and copy the ‘fruitDetectionTrackingCounting.m’ into the folder.<br>4 . Change the path to the folder.<br>4 . Load the ‘pal2.mat’ which contains the Faster RCNN detector.<br>4 . Input ‘global pal2’ in command window.<br>5 . Open and run the ‘FruitDetectionTrackingCounting’ script.</p><ul><li>important: Before running the script, please make sure you have already input ‘global pal2’ in command window, otherwise an exception will occur<br>‘function ‘detect’ for uint 8 variables are not defined.</li><li>Note: Since maximum size for submission files on wattle is 200MB, some large but important materials need to be download from goole driver.</li></ul><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h5 id="Visually-Demo"><a href="#Visually-Demo" class="headerlink" title="Visually Demo"></a>Visually Demo</h5><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=58801015&cid=102520996&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><br><br></p><h5 id="Accuracy-of-this-model"><a href="#Accuracy-of-this-model" class="headerlink" title="Accuracy of this model"></a>Accuracy of this model</h5><div align="center"><img src="/2019/07/11/FriutDetectionTrackingandCounting/table.png"></div><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><hr><p>We used Transfer Learning, Alexnet network, Faster-RCNN, Kalman filtert, and others techology to implement this project and have tested all provided videos. The performance is shown in the report ‘Fruit Detection, Tracking and Counting_Team 4.pdf’.</p><h1 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h1><p> [1]Motion-Based-Multiple-object-tracking, <a href="https://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html" target="_blank" rel="noopener">https://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html</a><br> [2] A. Krizhevsky, I. Sutskever and G. Hinton, “ImageNet classification with deep convolutional neural networks,” Communications of the ACM, vol.60, (6), pp. 84-90, 2017. . DOI: 10.1145/3065386.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2019/07/11/Linear-Regression/"/>
      <url>/2019/07/11/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Basically we can generally classify the linear regression as two different types.</p><ul><li>Linear regression with one variable</li><li>Linear regression with multiple variables</li></ul><a id="more"></a><p>1.For the first type of Linear regression, it only has one factor and a coresponding label. As shown in the forllowing figure[2], the size of the house is the variable and the price is so-called label. What we wanna do is to find a line or curve to fit the data points and then estimate the price with given a new data point.</p><div align="center"><img src="/2019/07/11/Linear-Regression/houseprice.jpg"></div><br><p>2.Similarly , for the second type of Linear regression, it has mutiple variables to determine the housing price, such as the size, transportaion, schools, etc. Instead of using the single variable described above, a feature matrix is utilized in Linear regression.</p><p>So, this is the basic concept of liear regression with one variable.</p><h2 id="Mathematical-Principle"><a href="#Mathematical-Principle" class="headerlink" title="Mathematical Principle"></a>Mathematical Principle</h2><p>This section is going to elaborate the hidden principle of Linear regression. At the begining, we need to make some denotation for effective description. The objective of Linear regression is to find a line or curve that fits data and then to estimate with given a new input.</p><h5 id="Modelling"><a href="#Modelling" class="headerlink" title="Modelling"></a>Modelling</h5><p>Simply denoting the input as :</p><p>$$x = [x_1,x_2,x_3,…,x_m]^T$$<br>note: $x_m$ here represnts a feature vector of the $m_{th}$ sample and the label is denoting as:<br>$$y = [y_1,y_2,y_3,…,y_m]^T$$</p><p>what we would like to do is to find a line :</p><p>$$h_{w}(x) = w_0 + w_1 x_1 + w_2 x_2 + …. + w_m x_m$$</p><p>Therefore, we need to find the parameters $w$ to obtain a good line or curve and then do estimation.<br>The beautiful expectation is that the estimated value is close to the truth value as possible as it can. In other words, the target is to minimize the distance between predicted value and truth value. Hence, we introude a function called “Cost function”, which is also named “Square Error function”.:</p><p>$$ J(w)=\frac{1}{2m}\sum_{i=1}^m (h_{w}(x_{(i)}) - y_{(i)})^2$$</p><p>$h_{w}(x_{(i)})$  denotes the predition of this model. As described before, the objective is to minimize the error by figuring out the parameters $w$.</p><h5 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h5><p>Obviously that the cost function is a convex function, so that we just take a derivative with respect to the $w$ and then let its derivative equal to zero to obtain the weight at the local minimum point. Next, we need to update the weight, $w$ by traversing the whole data set from $x_1$ to $x_m$ using forllowing equation:<br>$$w_i = w_i - \alpha \frac{d}{dw}J(w)$$<br>$\alpha$ is represented as learning rate.<br>This method is named as gradient descent algorithm, which will continuously update the weight and then the cost function will gradually step to the minimum point.</p><h2 id="Statistically-description"><a href="#Statistically-description" class="headerlink" title="Statistically description"></a>Statistically description</h2><h5 id="Minimum-Square-Error"><a href="#Minimum-Square-Error" class="headerlink" title="Minimum Square  Error"></a>Minimum Square  Error</h5><p>continuously using above denotation, we introude the Minimum Square Error(MSE) algorithm. We have a cost function and to simplfy the computation, we omit the constant scaling varaibale  $\frac{1}{2m}$ and replace $h_w(x_{(i)})$ with $w^Tx_i$<br>$$ J(w)=\sum_{i=1}^m ||t_i - y_i||^2=\sum_{i=1}^m (w^Tx_i- y_i)^2$$<br>where $t_i$ is the output of the mdoel.<br>Therefore, we have</p><div align="center"><img src="/2019/07/11/Linear-Regression/formula.jpg"></div><p>$$X^TXW=X^TY  $$<br>Finally, we obtatin:<br>$$W^* =(X^TX)^{-1}X^TY$$<br>Or we can directly compute the weight, $w$, in this way:<br>$$J(w)=||t_i-y_i||^2$$<br>Take the Gradient<br>$$\frac{d}{dw}J(w)=2(XW-Y)^TX$$<br>solve for stationary point<br>$$X^TXW=X^TY$$</p><p><br><br></p><h5 id="Maximum-likelihood"><a href="#Maximum-likelihood" class="headerlink" title="Maximum likelihood"></a>Maximum likelihood</h5><p>From the Probability persepctive, we supoose that $ \epsilon \thicksim N(0, \sigma^2)$，just ignoring bias term $b$ here for simpicity. Hence, $ t=y(x,w)+\epsilon=w^Tx+\epsilon$. Therefore, what we wanna do is to maximise the prediction probability, $p(t|x,w)$, given data and weight. It is apparent that:<br>$$p(t|x,w)\thicksim N(w^T,\sigma^2)$$<br>$$p(t|x,w)=\frac{1}{\sqrt{2\pi}\sigma} exp[-\frac{(t-w^Tx)^2}{2\sigma^2}]<br>$$<br>We definitely aim to find the largest probability, which means that we are looking for a method that maximum likelihood. There are two persepctives to solve this problem.</p><ul><li>Frequentis Aprroach</li><li>Bayesian Approach</li></ul><ol><li>Frequentis Approach<br>Maximum likelihood Estimate (MLE):<br>$$J(w)=logp(t|x,w)=log\prod_{i=1}^mp(t_i|x_i,w)$$<br>$$=\sum_{i=1}^mlogp(t_i|x_i,w) =\sum_{i=1}^m\left(log\frac{1}{\sqrt{2\pi}\sigma}+logexp[-\frac{(t_i-w^Tx_i)^2}{2\sigma^2}]\right)$$<br>$$=\sum_{i=1}^m\left(log\frac{1}{\sqrt{2\pi}\sigma}+\frac{1}{2\sigma^2}(t_i-w^Tx_i)^2\right)$$<br>$\Rightarrow$<br>$$\hat{w}=argmax_wJ(w)=argmax_w\sum_{i=1}^m(-\frac{1}{2\sigma^2}(t_i-w^Tx_i)^2) = argmin_w\sum_{i=1}^m(t_i-w^Tx_i)^2$$<br>so far, the above equation is the same as shown in minimum cost function, therefore just taking gradient and then equalling to zero, then we can obtain<br>$$\hat{w}=(X^TX)^{-1}X^TY$$</li></ol><ol start="2"><li>Bayesian Approach (MAP)<br>Similarly at start, we supoose that $w\thicksim N(0,\beta^2)$, so we have:<br>$$p(w)=\frac{1}{\sqrt{2\pi}\beta^2}exp[-\frac{||w||^2}{2\beta^2}]<br>$$<br>and we aslo have:<br>$$p(t|x,w)=\frac{1}{({2\pi})^{\frac{1}{2}}\sigma}exp [-\frac{(t-w^Tx)^2}{2\sigma^2} ] $$<br>Therefore, according to the Bayesian rules:<br>$$p(w|t)=\frac{p(t|w)p(w)}{p(t)}$$<br>The term, $p(w|t)$, is posterior,  $p(t|w)$ is likelihood, $p(w)$ is prior.<br>we wanna maximum the likelihood and the posterior is proportional to  likelihood. So we have a method : Maximum a Posterior Estimation (MAP):<br>$$\hat{w}=argmax_wp(w|t) =argmax_wp(t|w)\bullet p(w)$$<br>$$=argmax_wlog(p(t|w)\bullet p(w))$$<br>$$=argmax_w\left(log(\frac{1}{\sqrt{2\pi}\sigma}\bullet \frac{1}{\sqrt{2\pi}\beta})+log exp [-\frac{(t-w^Tx)^2}{2\sigma^2}-\frac{||w||^2}{2\beta^2} ] \right)$$<br>so, deleting the first term, constant variables, we have:<br>$$argmin_wlog(\frac{(t-w^Tx)^2}{2\sigma^2}+\frac{||w||^2}{2\beta^2})$$<br>$$  =argmin_w\sum_{i=1}^{m}\left((t_i-w^Tx_i)^2+\frac{\sigma^2}{\beta^2}||w||^2\right)$$<br>Therefore, it finally turns into the minimum cost function and using above method, we can have the same result:<br>$$\hat{w}=(X^TX)^{-1}X^TY$$</li></ol><h5 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h5><p>However, you might be confused that what the second term is in above equation. Here we need to introude a term “regularization”. When we fit the data using a line or curve, probably it might overfit the data, let say what that mean[2]:</p><div align="center"><img src="/2019/07/11/Linear-Regression/regularization.jpg"></div><br><p>The above figure has three types of fitting, coresponding “underfitting”, “just right”, and “overfitting” respectively. The left one can not represent the data well while the right one fit the data perfectly, which the ability of prediction is relatively worse. Therefore, to address this problem, we add a regularized term to avoid overfitting. It works well when we have a lot of feature, each of which contributes a bit to predicting $t$ because it remains all the features but just reduces the magnitude of the parameters $w$. This is the fundamental idea of regularization. Therefore, changed cost function can be:<br>$$<br>J(w)=\frac{1}{2m}[\sum_{i=1}^m(t_i-w^Tx_i)^2+\lambda\sum_{i=1}^mw_i^2]<br>$$<br>where $\lambda$ is so-called regularization parameter. Apparently as increasing the $\lambda$, all the parameters, $w$, decrease. But if $\lambda$ is too large, the weight $w$ will be approximately to 0, then we will have a line paralleling to x axis. So, we need to have a reasonable magnitude of $\lambda$.<br>We also can obviously see that the $\lambda$ here is equal to $\frac{\sigma^2}{\beta^2}$.<br>So far, we have already known the cost function with regularization term so that we can minimum the cost function. Ignoring the constant varaibale for simplicity:<br>$$<br>J(w)=\sum_{i=1}^m||t_i-w^Tx_i||^2+\lambda w^Tw $$<br>$$<br> =w^TX^TXw-2w^TX^TY+Y^TY+\lambda w^Tw<br>$$<br>$$<br>=w^T(X^TX+\lambda)w-2w^TX^TY+Y^TY<br>$$<br>$\Rightarrow$<br>$$<br>\hat{w}=argmin_wJ(w)\Rightarrow\frac{d}{dw}J(w)=2(X^TX+\lambda)w-2X^TY=0$$<br>$$<br> \therefore \hat{w}=(X^TX+\lambda I)^{-1}X^TY<br>$$</p><p><br><br><br></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Before wraping up the Linear regression section, let’s draw a figure to show the relationship among the LSE, MLE and MAP discussed above.</p><div align="center"><img src="/2019/07/11/Linear-Regression/relationship.jpg"></div><br><p><br><br></p><h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><p>[1] 黄海广, <a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">https://github.com/fengdu78/deeplearning_ai_books</a><br>[2] Andrew Ng, <a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a><br>[3] shuhuai007 <a href="https://github.com/shuhuai007/Machine-Learning-Session" target="_blank" rel="noopener">https://github.com/shuhuai007/Machine-Learning-Session</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>My First Post</title>
      <link href="/2019/07/10/my-first-post/"/>
      <url>/2019/07/10/my-first-post/</url>
      
        <content type="html"><![CDATA[<div align="center"><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=1868008226,1930371026&fm=26&gp=0.jpg"><a id="more"></a><p>END</p></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/07/10/hello-world/"/>
      <url>/2019/07/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
