<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ElasticFusion</title>
      <link href="/2019/07/13/ElasticFusion/"/>
      <url>/2019/07/13/ElasticFusion/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Shallow-Neural-Network</title>
      <link href="/2019/07/13/Shallow-Neural-Network/"/>
      <url>/2019/07/13/Shallow-Neural-Network/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This blog will introduce the simple neural networks, showing the neural network represntation, computing a neural network’s output, and indicating what is activation function and why we need a non-linear activation function. Besides that, the backpropagation will be introduced as well.</p><a id="more"></a><ul><li>Neural network representation</li><li>Computing the output of the neural network</li><li>Activation function</li><li>Backpropagation</li></ul><h2 id="Neural-networks-representation"><a href="#Neural-networks-representation" class="headerlink" title="Neural networks representation"></a>Neural networks representation</h2><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/representation.jpg"></div><br><p>Simply say that we have input $x_1, x_2, x_3$ and have one hidden layer with four neural units and finally we have one output unit in above firgure showing a simple neural network architecture.</p><h2 id="Computing-the-output-of-the-neural-network-Feedforward"><a href="#Computing-the-output-of-the-neural-network-Feedforward" class="headerlink" title="Computing the output of the neural network (Feedforward)"></a>Computing the output of the neural network (Feedforward)</h2><p>In order to figure out the output of the neural network, let’s make some denotation.<br>Input: $x_1, x_2, x_3$. weight: $[w_1^{[1]},w_2^{[1]},w_3^{[1]},…,w_3^{[4]}$. Therefore we have:<br>$$<br>z=w^Tx+b<br>$$where term “b” is bias term. For example,we can obtain: $z_1^{[1]}=w_1^{[1]T}x+b$. And so on and so forth.<br>$$<br>z_2^{[1]}=w_2^{[1]T}x+b\\<br>z_3^{[1]}=w_3^{[1]T}x+b\\<br>z_4^{[1]}=w_4^{[1]T}x+b<br>$$<br>and then pass through the non-linear function “sigmoid function”:<br>$$a_1^{[1]}=\sigma(z_1^{[1]})\\<br>a_2^{[1]}=\sigma(z_2^{[1]})\\<br>a_3^{[1]}=\sigma(z_3^{[1]})\\<br>a_4^{[1]}=\sigma(z_4^{[1]})<br>$$<br>and then multiply the weight agian and pass through the sigmoid function:<br>$$<br>z_1^{[2]}=w_1^{[2]T}a^{[1]}+b\\<br>a_1^{[2]}=\sigma(z_1^{[2]})<br>$$<br>Therefore, $ \hat{y}= \ a_1^{[2]} $ in this case.</p><h2 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h2><p>What we use above is so-called activation functions. If there is no activation function in neural network, the output of the model is only the linear combination with input features x. Therefore, we can not use any linear activation function among hidden layers, but can use the linear activation in the output layer, which is the special case.<br>There are severl common used activation function, like “sigmoid”, “tanh”, “Relu”, and “leaky Relu”.</p><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/activation.jpg"></div><br><ul><li>sigmoid:<br>$$<br>a=\sigma(z)=\frac{1}{1+e^{-z}}\\<br>\frac{d}{dz}\sigma(z)=\sigma(z)(1-\sigma(z))<br>$$</li><li>tanh:<br>$$<br>a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\<br>\frac{d}{dz}tanh(z)=1-(tanh(z))^2<br>$$</li><li>Relu:<br>$$<br>a=max(0,z)$$</li></ul><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/Relu.jpg"></div><br><ul><li>leaky Relu<br>$$<br>a=max(0.01z,z)$$</li></ul><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/Leaky Relu.jpg"></div><br><p>The common disadvantage of sigmoid and tanh is that when the case where $z$ is too large or too small happens, the derivative of the function will be enormously small, even approximate to zero, which will decrease the speed of gradient descent. Alternatively, the  activation function “Relu” is also a popular choice in machine learning. The advantage of “Relu” is that the derivative is zero when the z is negative number. As for “Leaky Relu”, when z is negative number, the magnitude of this function is not equal to zero, rather slightly incline.<br>The experience to select the activation function:</p><ul><li>sigmoid: output is the binary classification</li><li>tanh: almost suitable for most situation</li><li>Relu: the most common defaut activation function</li></ul><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2>]]></content>
      
      
      
        <tags>
            
            <tag> DEEP LEARNING </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Change-detection-using-GMM</title>
      <link href="/2019/07/12/Change-detection-using-GMM/"/>
      <url>/2019/07/12/Change-detection-using-GMM/</url>
      
        <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Change detection is one of the most commonly used methods for detecting moving objects in a computer vision system. It detects moving objects in an image sequence by comparing the background model with the current frame. Traditional moving target detection algorithms have problems such as dynamic background changes caused by illumination changes, noise and shadow sensitivity.</p><a id="more"></a><p>In this project, a method based on Gaussian Mixture Model (GMM) is introduced for the challenge of object detection, which modeling each pixel of each frame as a mixture of Gaussians and using an on-line approximation to update the model. Based on whether the Gaussian distribution that represents it most effectively is considered part of the background model, each pixel is classified. This report is targeted to utilizes the python to implement the GMM algorithm to segment background for several videos<br><br><br>Repo: <a href="https://github.com/TianchaoHuo/Change-Detection-Using-GMM" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Change-Detection-Using-GMM</a></p><h2 id="Objective"><a href="#Objective" class="headerlink" title="Objective:"></a>Objective:</h2><p>1.Detecting the moving object in videos recorded by a stationary camera<br>2.Segmentation between foreground and background<br>3.Adaptive in working conditions<br>4.Not costly develop</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>PETS 2009 (<a href="ftp://pets.rdg.ac.uk" target="_blank" rel="noopener">ftp://pets.rdg.ac.uk</a>)</p><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><p>This implementataion is based on the python 3 and open-cv package.<br>Hence, you need to install the related implementation environment.</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=58925991&cid=102725017&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>Pros</li></ul><ol><li>Be adaptive to different scenes</li><li>Be capable to segment videos</li><li>Good trade-off between quality and execution time</li></ol><ul><li>Cons</li></ul><ol><li>Arbitrarily set the parameters</li><li>Assume each pixel is independent of its neighbors.</li></ol><p>More hidden principle of GMM or details of this project, please look through the project report.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] C. Stauffer and W. E. L. Grimson, “Adaptive background mixture models for real-time tracking,” in 1999, . DOI: 10.1109/CVPR.1999.784637.<br>[2] H. Shih-Shinh, <a href="https://www.youtube.com/watch?v=g_ve2txPkSc" target="_blank" rel="noopener">https://www.youtube.com/watch?v=g_ve2txPkSc</a>, 2018.<br>[3] Swapnil Das, <a href="https://github.com/swapnil96" target="_blank" rel="noopener">https://github.com/swapnil96</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Friut Detection, Tracking and Counting</title>
      <link href="/2019/07/11/FriutDetectionTrackingandCounting/"/>
      <url>/2019/07/11/FriutDetectionTrackingandCounting/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><hr><p>This project is designed to do fruit detection, traking and counting. Videos are already provided.</p><p>Co-Author: Chaohuo Wu, Xiaochen Liu, Do Kyoun Lee</p><p>Repo: <a href="https://github.com/TianchaoHuo/Fruit-Detection" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Fruit-Detection</a></p><a id="more"></a><h1 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h1><hr><p> 1 . Read in a video clip, and display the video frame by frame,<br> 2 . Detect all lemons and bananas in each frame, segment the corresponding pixel regions, count the number of pixels, draw a bounding box on these detected fruits.<br> 3 . Track the detected oranges and bananas in the entire video sequence, until they disappear.<br> 4 . Count the number of fruits of each type, and update the counting result in real-time.<br> 5 . At the end of each video clip, report the success rate of your program.</p><h1 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h1><hr><p>Open the video named ‘Test.mp4’. This is the one of the testing video.  </p><h1 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h1><hr><p>1 . DownLoad the project material from google driver. The link is: <a href="https://drive.google.com/open?id=1yQXSwYu-GljAxXZLB0YlPoV0lx7KVMPh" target="_blank" rel="noopener">https://drive.google.com/open?id=1yQXSwYu-GljAxXZLB0YlPoV0lx7KVMPh</a><br>2 . Open MATLAB software.<br>3 . Decompress the ‘project material.zip’ and copy the ‘fruitDetectionTrackingCounting.m’ into the folder.<br>4 . Change the path to the folder.<br>4 . Load the ‘pal2.mat’ which contains the Faster RCNN detector.<br>4 . Input ‘global pal2’ in command window.<br>5 . Open and run the ‘FruitDetectionTrackingCounting’ script.</p><ul><li>important: Before running the script, please make sure you have already input ‘global pal2’ in command window, otherwise an exception will occur<br>‘function ‘detect’ for uint 8 variables are not defined.</li><li>Note: Since maximum size for submission files on wattle is 200MB, some large but important materials need to be download from goole driver.</li></ul><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h5 id="Visually-Demo"><a href="#Visually-Demo" class="headerlink" title="Visually Demo"></a>Visually Demo</h5><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=58801015&cid=102520996&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><br><br></p><h5 id="Accuracy-of-this-model"><a href="#Accuracy-of-this-model" class="headerlink" title="Accuracy of this model"></a>Accuracy of this model</h5><div align="center"><img src="/2019/07/11/FriutDetectionTrackingandCounting/table.png"></div><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><hr><p>We used Transfer Learning, Alexnet network, Faster-RCNN, Kalman filtert, and others techology to implement this project and have tested all provided videos. The performance is shown in the report ‘Fruit Detection, Tracking and Counting_Team 4.pdf’.</p><h1 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h1><p> [1]Motion-Based-Multiple-object-tracking, <a href="https://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html" target="_blank" rel="noopener">https://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html</a><br> [2] A. Krizhevsky, I. Sutskever and G. Hinton, “ImageNet classification with deep convolutional neural networks,” Communications of the ACM, vol.60, (6), pp. 84-90, 2017. . DOI: 10.1145/3065386.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2019/07/11/Linear-Regression/"/>
      <url>/2019/07/11/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Basically we can generally classify the linear regression as two different types.</p><ul><li>Linear regression with one variable</li><li>Linear regression with multiple variables</li></ul><a id="more"></a><p>1.For the first type of Linear regression, it only has one factor and a coresponding label. As shown in the forllowing figure[2], the size of the house is the variable and the price is so-called label. What we wanna do is to find a line or curve to fit the data points and then estimate the price with given a new data point.</p><div align="center"><img src="/2019/07/11/Linear-Regression/houseprice.jpg"></div><br><p>2.Similarly , for the second type of Linear regression, it has mutiple variables to determine the housing price, such as the size, transportaion, schools, etc. Instead of using the single variable described above, a feature matrix is utilized in Linear regression.</p><p>So, this is the basic concept of liear regression with one variable.</p><h2 id="Mathematical-Principle"><a href="#Mathematical-Principle" class="headerlink" title="Mathematical Principle"></a>Mathematical Principle</h2><p>This section is going to elaborate the hidden principle of Linear regression. At the begining, we need to make some denotation for effective description. The objective of Linear regression is to find a line or curve that fits data and then to estimate with given a new input.</p><h3 id="Modelling"><a href="#Modelling" class="headerlink" title="Modelling"></a>Modelling</h3><p>Simply denoting the input as :</p><p>$$x = [x_1,x_2,x_3,…,x_m]^T$$<br>note: $x_m$ here represnts a feature vector of the $m_{th}$ sample and the label is denoting as:<br>$$y = [y_1,y_2,y_3,…,y_m]^T$$</p><p>what we would like to do is to find a line :</p><p>$$h_{w}(x) = w_0 + w_1 x_1 + w_2 x_2 + …. + w_m x_m$$</p><p>Therefore, we need to find the parameters $w$ to obtain a good line or curve and then do estimation.<br>The beautiful expectation is that the estimated value is close to the truth value as possible as it can. In other words, the target is to minimize the distance between predicted value and truth value. Hence, we introude a function called “Cost function”, which is also named “Square Error function”.:</p><p>$$ J(w)=\frac{1}{2m}\sum_{i=1}^m (h_{w}(x_{(i)}) - y_{(i)})^2$$</p><p>$h_{w}(x_{(i)})$  denotes the predition of this model. As described before, the objective is to minimize the error by figuring out the parameters $w$.</p><h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p>Obviously that the cost function is a convex function, so that we just take a derivative with respect to the $w$ and then let its derivative equal to zero to obtain the weight at the local minimum point. Next, we need to update the weight, $w$ by traversing the whole data set from $x_1$ to $x_m$ using forllowing equation:<br>$$w_i = w_i - \alpha \frac{d}{dw}J(w)$$<br>$\alpha$ is represented as learning rate.<br>This method is named as gradient descent algorithm, which will continuously update the weight and then the cost function will gradually step to the minimum point.</p><h2 id="Statistical-description"><a href="#Statistical-description" class="headerlink" title="Statistical description"></a>Statistical description</h2><h3 id="Minimum-Square-Error"><a href="#Minimum-Square-Error" class="headerlink" title="Minimum Square  Error"></a>Minimum Square  Error</h3><p>continuously using above denotation, we introude the Minimum Square Error(MSE) algorithm. We have a cost function and to simplfy the computation, we omit the constant scaling varaibale  $\frac{1}{2m}$ and replace $h_w(x_{(i)})$ with $w^Tx_i$<br>$$ J(w)=\sum_{i=1}^m ||t_i - y_i||^2=\sum_{i=1}^m (w^Tx_i- y_i)^2$$<br>where $t_i$ is the output of the mdoel.<br>Therefore, we have</p><div align="center"><img src="/2019/07/11/Linear-Regression/formula.jpg"></div><p>$$X^TXW=X^TY  $$<br>Finally, we obtatin:<br>$$W^* =(X^TX)^{-1}X^TY$$<br>Or we can directly compute the weight, $w$, in this way:<br>$$J(w)=||t_i-y_i||^2$$<br>Take the Gradient<br>$$\frac{d}{dw}J(w)=2(XW-Y)^TX$$<br>solve for stationary point<br>$$X^TXW=X^TY$$</p><p><br><br></p><h3 id="Maximum-likelihood"><a href="#Maximum-likelihood" class="headerlink" title="Maximum likelihood"></a>Maximum likelihood</h3><p>From the Probability persepctive, we supoose that $ \epsilon \thicksim N(0, \sigma^2)$，just ignoring bias term $b$ here for simpicity. Hence, $ t=y(x,w)+\epsilon=w^Tx+\epsilon$. Therefore, what we wanna do is to maximise the prediction probability, $p(t|x,w)$, given data and weight. It is apparent that:<br>$$p(t|x,w)\thicksim N(w^T,\sigma^2)$$<br>$$p(t|x,w)=\frac{1}{\sqrt{2\pi}\sigma} exp[-\frac{(t-w^Tx)^2}{2\sigma^2}]<br>$$<br>We definitely aim to find the largest probability, which means that we are looking for a method that maximum likelihood. There are two persepctives to solve this problem.</p><ul><li>Frequentis Aprroach</li><li>Bayesian Approach</li></ul><ol><li>Frequentis Approach<br>Maximum likelihood Estimate (MLE):<br>$$J(w)=logp(t|x,w)=log\prod_{i=1}^mp(t_i|x_i,w)$$<br>$$=\sum_{i=1}^mlogp(t_i|x_i,w) =\sum_{i=1}^m\left(log\frac{1}{\sqrt{2\pi}\sigma}+logexp[-\frac{(t_i-w^Tx_i)^2}{2\sigma^2}]\right)$$<br>$$=\sum_{i=1}^m\left(log\frac{1}{\sqrt{2\pi}\sigma}+\frac{1}{2\sigma^2}(t_i-w^Tx_i)^2\right)$$<br>$\Rightarrow$<br>$$\hat{w}=argmax_wJ(w)=argmax_w\sum_{i=1}^m(-\frac{1}{2\sigma^2}(t_i-w^Tx_i)^2) = argmin_w\sum_{i=1}^m(t_i-w^Tx_i)^2$$<br>so far, the above equation is the same as shown in minimum cost function, therefore just taking gradient and then equalling to zero, then we can obtain<br>$$\hat{w}=(X^TX)^{-1}X^TY$$</li></ol><ol start="2"><li>Bayesian Approach (MAP)<br>Similarly at start, we supoose that $w\thicksim N(0,\beta^2)$, so we have:<br>$$p(w)=\frac{1}{\sqrt{2\pi}\beta^2}exp[-\frac{||w||^2}{2\beta^2}]<br>$$<br>and we aslo have:<br>$$p(t|x,w)=\frac{1}{({2\pi})^{\frac{1}{2}}\sigma}exp [-\frac{(t-w^Tx)^2}{2\sigma^2} ] $$<br>Therefore, according to the Bayesian rules:<br>$$p(w|t)=\frac{p(t|w)p(w)}{p(t)}$$<br>The term, $p(w|t)$, is posterior,  $p(t|w)$ is likelihood, $p(w)$ is prior.<br>we wanna maximum the likelihood and the posterior is proportional to  likelihood. So we have a method : Maximum a Posterior Estimation (MAP):<br>$$\hat{w}=argmax_wp(w|t) =argmax_wp(t|w)\bullet p(w)$$<br>$$=argmax_wlog(p(t|w)\bullet p(w))$$<br>$$=argmax_w\left(log(\frac{1}{\sqrt{2\pi}\sigma}\bullet \frac{1}{\sqrt{2\pi}\beta})+log exp [-\frac{(t-w^Tx)^2}{2\sigma^2}-\frac{||w||^2}{2\beta^2} ] \right)$$<br>so, deleting the first term, constant variables, we have:<br>$$argmin_wlog(\frac{(t-w^Tx)^2}{2\sigma^2}+\frac{||w||^2}{2\beta^2})$$<br>$$  =argmin_w\sum_{i=1}^{m}\left((t_i-w^Tx_i)^2+\frac{\sigma^2}{\beta^2}||w||^2\right)$$<br>Therefore, it finally turns into the minimum cost function and using above method, we can have the same result:<br>$$\hat{w}=(X^TX)^{-1}X^TY$$</li></ol><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>However, you might be confused that what the second term is in above equation. Here we need to introude a term “regularization”. When we fit the data using a line or curve, probably it might overfit the data, let say what that mean[2]:</p><div align="center"><img src="/2019/07/11/Linear-Regression/regularization.jpg"></div><br><p>The above figure has three types of fitting, coresponding “underfitting”, “just right”, and “overfitting” respectively. The left one can not represent the data well while the right one fit the data perfectly, which the ability of prediction is relatively worse. Therefore, to address this problem, we add a regularized term to avoid overfitting. It works well when we have a lot of feature, each of which contributes a bit to predicting $t$ because it remains all the features but just reduces the magnitude of the parameters $w$. This is the fundamental idea of regularization. Therefore, changed cost function can be:<br>$$<br>J(w)=\frac{1}{2m}[\sum_{i=1}^m(t_i-w^Tx_i)^2+\lambda\sum_{i=1}^mw_i^2]<br>$$<br>where $\lambda$ is so-called regularization parameter. Apparently as increasing the $\lambda$, all the parameters, $w$, decrease. But if $\lambda$ is too large, the weight $w$ will be approximately to 0, then we will have a line paralleling to x axis. So, we need to have a reasonable magnitude of $\lambda$.<br>We also can obviously see that the $\lambda$ here is equal to $\frac{\sigma^2}{\beta^2}$.<br>So far, we have already known the cost function with regularization term so that we can minimum the cost function. Ignoring the constant varaibale for simplicity:<br>$$<br>J(w)=\sum_{i=1}^m||t_i-w^Tx_i||^2+\lambda w^Tw $$<br>$$<br> =w^TX^TXw-2w^TX^TY+Y^TY+\lambda w^Tw<br>$$<br>$$<br>=w^T(X^TX+\lambda)w-2w^TX^TY+Y^TY<br>$$<br>$\Rightarrow$<br>$$<br>\hat{w}=argmin_wJ(w)\Rightarrow\frac{d}{dw}J(w)=2(X^TX+\lambda)w-2X^TY=0$$<br>$$<br> \therefore \hat{w}=(X^TX+\lambda I)^{-1}X^TY<br>$$</p><p><br><br><br></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Before wraping up the Linear regression section, let’s draw a figure to show the relationship among the LSE, MLE and MAP discussed above.</p><div align="center"><img src="/2019/07/11/Linear-Regression/relationship.jpg"></div><br><p>The next step is to implement these algorithm using python.</p><font size="4" face="Times New Roman"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.path = <span class="string">'ex1data1.txt'</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(self, X, y, theta)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(np.power((X*theta.T)-y),<span class="number">2</span>)/(<span class="number">2</span>*len(X))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(self, X, y theta, learningrate, epoch)</span>:</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>]) <span class="comment"># ravel flatten function</span></span><br><span class="line">    cost = np.zeros(epoch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">      error = (X * theta.T - y)</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error,X[:,j])</span><br><span class="line">        temp[<span class="number">0</span>,j] = theta[<span class="number">0</span>,j] - ((alpha/len(X))*np.sum(term))</span><br><span class="line"></span><br><span class="line">      theta = temp</span><br><span class="line">      cost[i] = self.computeCost(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T @ X) @ X.T @ y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">  Admin = LinearRegression()</span><br><span class="line">  data = pd.read_csv(Admin.path, header=<span class="literal">None</span>, names=&#123;<span class="string">'Population'</span>, <span class="string">'Profit'</span>&#125;)</span><br><span class="line">  data.plot(kind=<span class="string">'scatter'</span>, x=<span class="string">'Population'</span>, y=<span class="string">'Profit'</span>, figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">  plt.show()</span><br><span class="line">  <span class="comment"># adding a column of ones will achieve vectorization</span></span><br><span class="line">  data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line">  <span class="comment"># set X(training data) and y(target variable)</span></span><br><span class="line">  cols = data.shape[<span class="number">1</span>]</span><br><span class="line">  X = data.iloc[:, <span class="number">0</span>:cols<span class="number">-1</span>]</span><br><span class="line">  y = data.iloc[:, cols<span class="number">-1</span>:cols]</span><br><span class="line">  <span class="comment"># convert from data framse to numpy matrices</span></span><br><span class="line">  X = np.matrix(X.values)</span><br><span class="line">  y = np.matrix(y.values)</span><br><span class="line">  theta = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line">  <span class="comment"># set parameters and do calculation</span></span><br><span class="line">  alpha = <span class="number">0.01</span></span><br><span class="line">  epoch = <span class="number">1000</span></span><br><span class="line">  g,cost = Admin.gardientDescent(X, y, theta, alpha, epoch)</span><br><span class="line">  temp = Admin.computeCost(X, y, g)</span><br><span class="line">  print(g)</span><br><span class="line">  <span class="comment"># plotting the result</span></span><br><span class="line">  x = np.linspace(data.Population.min(), data.Population.max(), <span class="number">100</span>)</span><br><span class="line">  print(x.shape)</span><br><span class="line">  f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line">  print(f.shape)</span><br><span class="line">  fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">  ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">  ax.scatter(data.Population, data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">  ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">  ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">  ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">  ax.set_title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">  ax.plot(np.arange(epoch), cost, <span class="string">'r'</span>)</span><br><span class="line">  ax.set_xlabel(<span class="string">'Iterations'</span>)</span><br><span class="line">  ax.set_ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">  ax.set_title(<span class="string">'Error vs. Training Epoch'</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  <span class="comment">#  normalize equation</span></span><br><span class="line">  <span class="comment"># diretly using LSE equation insteady of gradient descent.</span></span><br><span class="line">  final_theta2 = Admin.normalEqn(X, y)</span><br><span class="line">  print(final_theta2)</span><br></pre></td></tr></table></figure></font><br><br><h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><p>[1] 黄海广, <a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">https://github.com/fengdu78/deeplearning_ai_books</a><br>[2] Andrew Ng, <a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a><br>[3] shuhuai007 <a href="https://github.com/shuhuai007/Machine-Learning-Session" target="_blank" rel="noopener">https://github.com/shuhuai007/Machine-Learning-Session</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>My First Post</title>
      <link href="/2019/07/10/my-first-post/"/>
      <url>/2019/07/10/my-first-post/</url>
      
        <content type="html"><![CDATA[<div align="center"><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=1868008226,1930371026&fm=26&gp=0.jpg"><a id="more"></a><p>END</p></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/07/10/hello-world/"/>
      <url>/2019/07/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
