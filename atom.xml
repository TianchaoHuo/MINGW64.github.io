<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Homepage</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-22T07:15:06.896Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Tianchao Huo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Logistic Regression</title>
    <link href="http://yoursite.com/2019/07/21/Logistic-Regression/"/>
    <id>http://yoursite.com/2019/07/21/Logistic-Regression/</id>
    <published>2019-07-21T12:54:03.000Z</published>
    <updated>2019-07-22T07:15:06.896Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This article explores the logistic regression, including its function, principle, mathematical formulas. Logistic regression has “sigmoid” function, allowing the samples map to a value in $[0, 1]$ to do classification problems. The difference between linear regression and logistic regression is that the former is used to prediction and the latter is used to classification.</p><a id="more"></a><h2 id="Principle"><a href="#Principle" class="headerlink" title="Principle"></a>Principle</h2><p>We have a data set $[(x_1,y_1),(x_2,y_2),…,(x_m,y_m)]$, $y\in{[0, 1]}$ and besides that, we have a definition of sigmoid function:<br>$$<br>\sigma(z)=\frac{1}{1+e^{-z}}<br>$$</p><div><img src="/2019/07/21/Logistic-Regression/logistic_properties.jpg"><img src="/2019/07/21/Logistic-Regression/logistic regression.jpg"></div>### cost function<p>And we have a <strong>cost function</strong> for logistic regression:</p><p>$$<br>J(w)=\sum_{i=1}^m [-y_ilog(t_i)-(1-y_i)log(1-t_i)]<br>$$<br>where $t_i$ is the prediction probability.<br>The reason is that:</p><div align="center"><img src="/2019/07/21/Logistic-Regression/relationship.jpg"></div><p>when the $y=1$ and the predition $t$ is equal to 1, then the cost function $J(w)=0$, but if $y=1$, $t\rightarrow 0$, then the cost function $J(w)\rightarrow \infty$ and similarly for $y=0$.</p><h3 id="gradient-descent"><a href="#gradient-descent" class="headerlink" title="gradient descent"></a>gradient descent</h3><p>It is known that the derivative of sigmoid function is :<br>$$<br>\sigma(z)=\sigma(z)(1-\sigma(z))<br>$$<br>Minimizing the cost function to obtain weights:<br>$$<br>w^* = argmin_wJ(w)<br>$$</p><p>So that taking a partial derivative of $J(w)$ with respect to $w$, we have:<br>$$<br>\frac{dJ}{dw}=-\frac{d}{dw}[\sum_{i=1}^m [y_ilog(t_i)+(1-y_i)log(1-t_i)]]\\<br>= -\sum_{i=1}^m\frac{d}{dw}[y_i\frac{1}{t_i}t_i(1-t_i)x_i+(1-y_i)\frac{1}{1-t_i}t_i(1-t_i)x_i]\\<br>=\sum_{i=1}^m(t_i-y_i)x_i<br>$$</p><h3 id="updating-the-weights"><a href="#updating-the-weights" class="headerlink" title="updating the weights"></a>updating the weights</h3><p>$$<br>w := w - \alpha\frac{dJ}{dw}<br>$$</p><p>where $\alpha$ is the learning rate.</p><h2 id="Statistics-model"><a href="#Statistics-model" class="headerlink" title="Statistics model"></a>Statistics model</h2><p>we hava likelihood function<br>$$<br>p(t|w)=\prod_{i=1}^mt_i^{y_i}(1-t_i)^{1-y_i}<br>$$<br>where $y_i=p(C_i)$, $C_i$ denotes the classes that this sample belongs to.<br>We wanet to maximize the probability $p(t|w)$, therefore:<br>$$<br>w^* = argmax_wlogp(t|w)\\= argmax_wlog(\prod_{i=1}^mt_i^{y_i}(1-t_i)^{1-y_i})\\=argmax_w\sum_{i=1}^mlog(t_i^{y_i}(1-t_i)^{1-y_i})\\=argmax_w\sum_{i=1}^m(y_ilog(t_i)+(1-y_i)log(1-t_i))<br>$$<br>the $-\sum_{i=1}^m(y_ilog(t_i)+(1-y_i)log(1-t_i))$ is called <strong>cross-entropy error.</strong><br><br><br>Gradient of the error function (using $\frac{d\sigma}{dz}=\sigma(z)(1-\sigma(z))$),<br>$$<br>\frac{dJ}{dw}=\sum_{i=1}^m(t_i-y_i)x_i<br>$$</p><ul><li>Graident does not contain any sigmoid function</li><li>for each data point error is product of deviation $(t_i-y_i)$ and $x_i$</li><li>Maximum likelihood solution can exhibit over-fitting even for many data points; should use regularised error or MAP then.</li></ul><p>And therefore <strong>Maximize likelihood estimate is equal to minimize the error function.</strong></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] 黄海广, <a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">https://github.com/fengdu78/deeplearning_ai_books</a> <br><br>[2] Andrew Ng, <a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a><br><br>[3] shuhuai007 <a href="https://github.com/shuhuai007/Machine-Learning-Session" target="_blank" rel="noopener">https://github.com/shuhuai007/Machine-Learning-Session</a><br><br>[4] sml, <a href="https://machlearn.gitlab.io/sml2019/" target="_blank" rel="noopener">https://machlearn.gitlab.io/sml2019/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;This article explores the logistic regression, including its function, principle, mathematical formulas. Logistic regression has “sigmoid” function, allowing the samples map to a value in $[0, 1]$ to do classification problems. The difference between linear regression and logistic regression is that the former is used to prediction and the latter is used to classification.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Autonomous driving application using YOLO</title>
    <link href="http://yoursite.com/2019/07/17/Autonomous-driving-application-using-YOLO/"/>
    <id>http://yoursite.com/2019/07/17/Autonomous-driving-application-using-YOLO/</id>
    <published>2019-07-17T09:30:27.000Z</published>
    <updated>2019-07-20T05:56:40.369Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>This assignment is to work out a system detecting cars in a video. To collect data, you can mount a camera to the hood(meaning the front) of the car, which takes pictures of the road every few seconds while you drive around. After gathering all these images in to a folder, we label them by drawing bounding boxes around every car we found. In this project, we use the data from <a href="https://www.drive.ai/" target="_blank" rel="noopener">drive.ai</a>.</p><a id="more"></a><p>repo: <a href="https://github.com/TianchaoHuo/Autonomous-driving-application-using-YOLO" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Autonomous-driving-application-using-YOLO</a></p><h2 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h2><p>We are target to detect the cars and build up bounding boxes for them. The bounding box should like as:</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/definition of a box.jpg"></div><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><ol><li>Training set is from :<a href="https://www.drive.ai/" target="_blank" rel="noopener">drive.ai</a></li><li>YOLO pre-trained neural network:<ul><li><a href="http://pjreddie.com/media/files/yolo.weights" target="_blank" rel="noopener">YOLO weights</a></li><li><a href="https://github.com/pjreddie/darknet/tree/master/cfg" target="_blank" rel="noopener">YOLO cfg</a></li><li><a href="https://github.com/allanzelener/YAD2K" target="_blank" rel="noopener">YAD2K</a></li></ul></li></ol><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Here I just briefly summary Object Detection delivered by Andrew Ng to make the logic of this assignment clearly.</p><h3 id="sliding-windows"><a href="#sliding-windows" class="headerlink" title="sliding windows"></a>sliding windows</h3><p>As for object detection, we firstly think about how to classify objects, meaning that input an image and output what is it. And then when we waner detect the object, localizing where is the object, we require the neural network to output the object coordinates, such as the centeral point and so on. One approach to do that is <strong>sliding windows</strong>.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/sliding windows.jpg"></div>  <p>As shown above figure, we initially have a red window in the left-most cornner and then crop this corresponding size of image, inputing the neural network, to work out if here has the interested object. Repeating this process by shifting this window, we can finally traverse all the snippets of the image. However, this method has an obvious drawback, which is the computation cost is high. Therefore, we move to <strong>convolutional sliding windows</strong>.</p><h2 id="convolutional-implementation-of-sliding-windows"><a href="#convolutional-implementation-of-sliding-windows" class="headerlink" title="convolutional implementation of sliding windows"></a>convolutional implementation of sliding windows</h2><p>In order to implement  convolutional sliding windows, we need to know how to change from  fully conneted layers to convolutional layers of a neural network.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/Turning FC layer into convolutional layers.jpg"></div>  <p>What we do is, as shown in above figure, to using 400 5x5 filters in fully conneted layer 1 so that we can have 1x1x400 output layer, instead of a vector of size 400. Similarly we use 400 1x1 filters and 4 1x1 filters in later fully conneted layers.<br>Next, we implement the sliding windows through convolution. Suppose that we have an input image with 16x16x3 and we use 5x5x3 windows to traverse this input image and then we can obtain the 12x12x16 ouput image after a convolutional process. Focusing on the step highlited by the red 4 superscript, we have four small grids, rather than only one, since we combine all the output into a big grid, representing the first convolutional output of the upper left area, the upper right, the  lower left and the lower right, separately. So, this principle of convolutional operation is that we do not need to divide the input image into 4 subsets, passing them through the forward propagation respectively.Instead we take this as an input image to feed to the neural network, as  2x2x4 shown here.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/convolution implementation of sliding windows.jpg"></div>  <p>But it still has a drawback like the bounding box is not perfectly accurate.</p><h3 id="Bounding-box-predictions"><a href="#Bounding-box-predictions" class="headerlink" title="Bounding box predictions"></a>Bounding box predictions</h3><p>Sliding windows method has a drawback, so-called disordered bounding boxes.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/disordered bboxes.jpg"></div>  <p>You might see that the sliding winodw can not perfectly localize the car, or even we can say that the box (number [1]) is the most perfect box in this case. To address this problem, we introduce the YOLO algortihm, also meaning that You only look once [1]. Supposing we have an input image 100x100. and then we set a 3x3 net grid for simplicity.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/Yolo algorithm.jpg"></div>  <p>we need to define a training set label such as shown in the above. $p_c$ denotes the probability having a interested object. $b_x,b_y,b_h,b_w$ represent the bounding box coordinate  and $c_1,c_2,c_3$ denote the probability of corresponding classes, like car, traffic lights, or pedestrian. For any one of this 9 grids, we can obtain a vector ouput of size 8x1, so that we can have 3x3x8 for total expected output size. The cental point of detected object will allow the object to be allocated into the grid in where the central point is, even though this object crosses through several grids. This works well if there is no more than one object in each grid. Besides, it also requires that the training data should have the same label as this.The advantages of YOLO algortihm is that outputs of a neural network include accurate bounding boxes.</p><h3 id="Intersection-over-union-Iou"><a href="#Intersection-over-union-Iou" class="headerlink" title="Intersection over union (Iou)"></a>Intersection over union (Iou)</h3><p>You might consider that the figure might has numerous bounding boxes for a single object as these snippets of a image are considered as interested objects by the sliding windows method, resulting redundant bounding boxes for a object. Therefore, Iou is set to measure the overlap between two bounding boxes.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/IOU1.jpg"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/IOU2.jpg"></div><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/IOU3.jpg"></div>  <p>Generally we would say the detection is correct if $Iou &gt; 0.5$ or alternatively you might choose $Iou &gt; 0.6$ is OK.<br>Next, I waner introduce the non-max suppression algorithm.</p><h3 id="non-max-suppression-algorithm"><a href="#non-max-suppression-algorithm" class="headerlink" title="non-max suppression algorithm"></a>non-max suppression algorithm</h3><p>The algorithm may detect the same object multiple times.Non-maximum suppression is a way to make sure that your algorithm only detects each object once. Let’s do an example.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/NON-MAX.jpg"></div>  It firstly selects the bounding box with the highest probability, in this case in 0.9 which is the most reliable detection, and then we highlite this bounding box. Next we compute the Iou between this highlited bounding box and the remaining bounding boxes  in sequence. By setting a threshold value for Iou, all the bounding boxes will be supressed if it has high Iou with the highlited bounding box. And then, the left bounding boxes will be assessed one by one to find the highest probability, in this case in 0.8. So that we can detect another car on the left and then do the non-max suppression algorithm again to omit the high Iou bounding boxes. Therefore, we can obtain the final result. This is basic concept of the non-max supression algorithm.<p>The following figure is to detail the algorithm:</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/nms.jpg"></div>  <div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/nms1.jpg"></div>  <div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/nms2.jpg"></div><h3 id="anchor-box"><a href="#anchor-box" class="headerlink" title="anchor box"></a>anchor box</h3><p>One of the problems with object detection so far is that each grid can only detect one object and if you want to one grid detects multiple objects, you can do this by using the concept of anchor box.</p><div align="center"><img src="/2019/07/17/Autonomous-driving-application-using-YOLO/anchor box.jpg"></div>You can enlarge the vector to fill the different object parameters with anchor box. For example, the first 8 parameters of y vector is for the anchor box1, which is denoting the humanbeing and the remaining 8 parameters is denoting the car, which is also anchor box2. Now each object is assigned to the same grid as before. People would arbitrarily select the shape of anchor box and you can choose five to ten shapes of anchor box.<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Basically I waner give some general workflow of this system.</p><ol><li>Define anchor, classes, image shape. These come from “yolo_anchors.txt”, “coco_classes.txt” and the image shape is (720,1280).</li><li>Loading the pre-trained model “Yolo.h5” file.</li></ol><ul><li>We firstly download the “<a href="http://pjreddie.com/media/files/yolo.weights" target="_blank" rel="noopener">yolo.weights</a>“, and then “<a href="https://github.com/pjreddie/darknet/tree/master/cfg" target="_blank" rel="noopener">yolov2.cfg</a>“ and “<a href="https://github.com/allanzelener/YAD2K" target="_blank" rel="noopener">YAD2K</a>“.</li><li>put “yolo.weights”, “yolo.cfg”(change name from yolov2.cfg to yolo.cfg) and “yad2k.py” into a floder and copy the model_data floader into this new floader.</li><li>copy the yad2k floader in your download YAD2K-master into this new floader.</li><li>avtivate your python environment that includes tensorflow</li><li>excute “python yad2k.py yolo.cfg yolo.weights model_data/yolo.h5”<br>you can refer to this <a href="https://blog.csdn.net/Solo95/article/details/85262828" target="_blank" rel="noopener">blog</a>.</li></ul><ol start="3"><li>Obtain the outputs from the pre-trained model Yolo</li><li>Prediction<ul><li>feed an input image to Yolo model</li><li>get the Yolo outputs (scores, boxes, classes)</li><li>yolo box to corners</li><li>yolo filter boxes</li><li>scale bboxes</li><li>non-max-suppression</li><li>showing the result</li></ul></li></ol><p><strong>Summary for YOLO</strong>: - Input image (608, 608, 3) - The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output. - After flattening the last two dimensions, the output is a volume of shape (19, 19, 425): - Each cell in a 19x19 grid over the input image gives 425 numbers. - 425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture. - 85 = 5 + 80 where 5 is because  (𝑝𝑐,𝑏𝑥,𝑏𝑦,𝑏ℎ,𝑏𝑤)  has 5 numbers, and and 80 is the number of classes we’d like to detect - You then select only few boxes based on: - Score-thresholding: throw away boxes that have detected a class with a score less than the threshold - Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes - This gives you YOLO’s final output.</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>We firstly and simply use one image to test the model as shown in the left below. And then the model will output the right below image drawing several bounding boxes and corresponding probability and classes.</p><div><img width="550" height="330" src="/2019/07/17/Autonomous-driving-application-using-YOLO/test.jpg"><img width="550" height="330" src="/2019/07/17/Autonomous-driving-application-using-YOLO/test_result.jpg"></div><p>And the visual demo looks like:<br><br></p><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=59559199&cid=103764069&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>This is simply impplement the Yolo algorithm. For the future work, it is expected to work with videos in real-time.<br><strong>What you should remember</strong>: - YOLO is a state-of-the-art object detection model that is fast and accurate - It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. - The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes. - You filter through all the boxes using non-max suppression. Specifically: - Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes - Intersection over Union (IoU) thresholding to eliminate overlapping boxes - Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - You Only Look Once: Unified, Real-Time Object Detection (2015)<br>[2] Joseph Redmon, Ali Farhadi - YOLO9000: Better, Faster, Stronger (2016)<br>[3] Allan Zelener - YAD2K: Yet Another Darknet 2 Keras<br>[4] The official YOLO website (<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">https://pjreddie.com/darknet/yolo/</a>)<br>[5] How to build Yolo.h5, <a href="https://blog.csdn.net/Solo95/article/details/85262828" target="_blank" rel="noopener">https://blog.csdn.net/Solo95/article/details/85262828</a><br>[6] Andrew Ng, <a href="https://mooc.study.163.com/term/2001392030.htm" target="_blank" rel="noopener">https://mooc.study.163.com/term/2001392030.htm</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;This assignment is to work out a system detecting cars in a video. To collect data, you can mount a camera to the hood(meaning the front) of the car, which takes pictures of the road every few seconds while you drive around. After gathering all these images in to a folder, we label them by drawing bounding boxes around every car we found. In this project, we use the data from &lt;a href=&quot;https://www.drive.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;drive.ai&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Computer Vision" scheme="http://yoursite.com/categories/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Computer-Vision/Deep-Learning/"/>
    
    
      <category term="Computer Vision" scheme="http://yoursite.com/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Style-Transfer</title>
    <link href="http://yoursite.com/2019/07/16/Style-Transfer/"/>
    <id>http://yoursite.com/2019/07/16/Style-Transfer/</id>
    <published>2019-07-16T07:50:24.000Z</published>
    <updated>2019-07-19T14:53:42.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Neural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks in order to perform the image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user supplied photographs. [1] In this project, we are going to implement this algorithm and it is also the assignment in the deep learning AI course delivered by Andrew Ng [2].</p><p>Repo: <a href="https://github.com/TianchaoHuo/neural-style-transfer" target="_blank" rel="noopener">https://github.com/TianchaoHuo/neural-style-transfer</a></p><a id="more"></a><h2 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h2><p>In this example, we are going to generate an image of the Louvre museum in Paris (content image C), mixed with a painting by Claude Monet, a leader of the impressionist movement (style image S) [2]. This is an expected result of our implementataion.</p><div align="center"><img src="/2019/07/16/Style-Transfer/objective.jpg"></div><br><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>For any neural model, it definitely has the cost function. Therefore, we define the cost function at the begining.</p><h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>We have two main parts of the cost function[3]:</p><ul><li>$J_{content}(C,G)$: this one is so-called content cost, which is related to the originial image(content image) and the generated image and is used to measure the similarity between the generated image and the content image.</li><li>$J_{style}(S,G)$: this one is named as style cost, realting to the style image and the generated image, which is target to value the similarity between the style image and the generated image.</li></ul><p>And then finally we put them together, so that we have:<br>$$J(G)=\alpha J_{content}(C,G) + \beta J_{content}(S,G)$$<br>$\alpha$ and $\beta$ are the hyperparameters.</p><p>In order to generate a new image, we firstly need to randomly inital an image $G$ and then using gradient descent to reduce above cost function so that we can updating the<br>$$<br>G := G-\frac{\alpha}{\alpha G}J(G)<br>$$<br>This step is acutally updating values of image G, rather than the weight by convention in classification we did before. So the above is an abstract of neural style transfer. Now we dive more into the cost function and elaborate the hidden principle.</p><h4 id="Content-cost-function"><a href="#Content-cost-function" class="headerlink" title="Content cost function"></a>Content cost function</h4><p>Now we want to test the similarity between the content image and the generated image. To achieve that, we suppose we have two activation values:$a^{[l][C]}$ and $a^{[l][G]}$. We would say if these two values are close to each other, then the image is similar to each other. So we define:<br>$$<br>J_{content}=\frac{1}{2}||a^{[l][C]}-a^{[l][G]}||^2<br>$$<br>It means that L2 norm of the difference of the values of particular $l$ layer of different images can represent the content similarity of two images.</p><h4 id="Style-cost-function"><a href="#Style-cost-function" class="headerlink" title="Style cost function"></a>Style cost function</h4><p>We suppose that $a_{i,j,k}^{[l]}$ is  the activation value of the $l$ layer and $i,j,k$ represent height, width, and the number of channels. Now we are going to construct a style matrix, $G_{kk^{‘}}^{[l][S]}$, which is the $n_c$ by $ n_c$ matrix for style image.<br>Gram matrix (style matrix):<br>$$<br>G_{kk^{‘}}^{[l][S]}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{i,j,k}^{[l][S]}a_{i,j,k^{‘}}^{[l][S]}$$<br>$k,k^{‘}$ here denote the coefficient of association between $k$ channel and the $k{‘}$ channel. So we just multiply these two activation value of the same position in corresponding channels and then $i$ and $j$ increase by the height and width respectively, which are $n_H^{[l]}$ and $n_W^{[l]}$<br>Similarly, we do the same operation for the generated image, so that we have:<br>$$<br>G_{kk^{‘}}^{[l][G]}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{i,j,k}^{[l][G]}a_{i,j,k^{‘}}^{[l][G]}<br>$$<br>Then  we put them together to set up the style cost function:<br>$$<br>J_{style}(S,G)=\frac{1}{(2*n_H^l n_W^l n_c^l)^2}\sum_k \sum_{k^{‘}}{||G_{kk^{‘}}^{[l][S]}-G_{kk^{‘}}^{[l][G]}||}_{F}^2<br>$$</p><p>So far we have captured the style from only one layer. We’ll get better results if we “merge” style costs from several different layers. So we can have style weight matrix and then combine the style costs for different layers as follows:</p><p>$$<br>J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)<br>$$</p><p>Right now we can compute the total cost function by using gradient descent method.</p><h3 id="Pre-trained-neural-networks"><a href="#Pre-trained-neural-networks" class="headerlink" title="Pre-trained neural networks"></a>Pre-trained neural networks</h3><p>It is known that CNN can capture the high level feature of images [3]. As shown in the following, content image is passed through the CNN and then the image representation can be obtained, feature mapping in other words and then almost can obtain the image similar to the originial after reconstructing. In particular, the results obtained by reconstruction of the first few layers are closer to the original image, and it also indicates that more details of the images are retained in the first few layers, because there is the pooling layer behind, so some information of tuning will be naturally discarded.</p><div align="center"><img src="/2019/07/16/Style-Transfer/flow.jpg"></div><p>In the cost function, we utilize particular layer to compute the cost function. However, pratically if $l$ is small, for example, 1, then the generated image will be quite similar to your content image, but if it is too high, then the generated image will lose something contour or some interesting things of the content image. Hence, we usually selet a middel layer, rather than a too small or too deep layer. And then we also take advantage of the pre-trained neural network to implement the neural style transfer algorithm and VGG-19 neural network is used in this project. You can download the <a href="http://www.vlfeat.org/matconvnet/pretrained/" target="_blank" rel="noopener">pre-trained neural model</a> . And the following figure is the architecture of the VGG-19 neural network [4].</p><div align="center"><img src="https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg"></div>We are not diving into the VGG-19 nerual network in this case.<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><div align="left"><img width="245," height="178," src="/2019/07/16/Style-Transfer/style.jpg"> <img width="255," height="178," src="/2019/07/16/Style-Transfer/test2.jpg"> <img width="255," height="178," src="/2019/07/16/Style-Transfer/generated_image.jpg"><br>The left-most image is the style image: The Starry Night, from Van Gogh. And the middel one is "zhuhai" from [5]. The right-most one is the generated artical image from our nerual style transfer algorithm.<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This artical simply introduce the neural style transfer. We can learn that instead of updating the weight of the nerual network, it is target to update the values of the image while gradient descenting which is the same principle to minimize the cost function we defined above. In practica, the pre-trained nerual network is usually utilized to implement this project. The future work is to implement this algorithm in videos.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Neural style transfer, <a href="https://en.wikipedia.org/wiki/Neural_Style_Transfer" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Neural_Style_Transfer</a><br>[2] Andrew Ng, <a href="https://mooc.study.163.com/term/2001392030.htm" target="_blank" rel="noopener">https://mooc.study.163.com/term/2001392030.htm</a><br>[3] L. Gatys, A. Ecker and M. Bethge, “A Neural Algorithm of Artistic Style,” Journal of Vision, vol. 16, (12), pp. 326, 2016.<br>[4] VGG-19, <a href="https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356" target="_blank" rel="noopener">https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356</a><br>[5] zhuhai, <a href="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1563285220753&amp;di=bac7ad611650d54e5467b69c5db89a40&amp;imgtype=0&amp;src=http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F4oialo8ibP6okPtUN4Jj4DQFm8xenjibPHtYv6wVFaRicqorp3ticzqYShD64ibbIOBVuRdMzU3OnGMlgVLI89AXpHFA%2F640%3Fwx_fmt%3Djpeg" target="_blank" rel="noopener">https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1563285220753&amp;di=bac7ad611650d54e5467b69c5db89a40&amp;imgtype=0&amp;src=http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F4oialo8ibP6okPtUN4Jj4DQFm8xenjibPHtYv6wVFaRicqorp3ticzqYShD64ibbIOBVuRdMzU3OnGMlgVLI89AXpHFA%2F640%3Fwx_fmt%3Djpeg</a></p></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;Neural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks in order to perform the image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user supplied photographs. [1] In this project, we are going to implement this algorithm and it is also the assignment in the deep learning AI course delivered by Andrew Ng [2].&lt;/p&gt;
&lt;p&gt;Repo: &lt;a href=&quot;https://github.com/TianchaoHuo/neural-style-transfer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/TianchaoHuo/neural-style-transfer&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ElasticFusion</title>
    <link href="http://yoursite.com/2019/07/13/ElasticFusion/"/>
    <id>http://yoursite.com/2019/07/13/ElasticFusion/</id>
    <published>2019-07-13T12:15:12.000Z</published>
    <updated>2019-07-19T14:54:04.917Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Demonstrating a novel method to real-time dense visual SLAM, this paper summaries that the system is able to capture comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using and RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. The method they used is applying local model-to-model surface loop closure optimisations as often as possible to stay closure to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency. <a id="more"></a>In the spirit of improving map quality as well as tracking accuracy and robustness, they furthermore explore a novel approach to real-time discrete light source detection, which is capable of detecting numerous light sources in indoor environments in realtime as a user handheld camera explores the scene. The results present that techniques they proposed function well in many different environments and lighting configurations and also enable more realistic augmented reality (AR) rendering, a richer understanding of the scene beyond pure geometry and more accurate and robust photometric tracking</p><p>Original Repo: <a href="https://github.com/mp3guy/ElasticFusion" target="_blank" rel="noopener">https://github.com/mp3guy/ElasticFusion</a></p><h2 id="Objective"><a href="#Objective" class="headerlink" title="Objective:"></a>Objective:</h2><p>This project aims to move away from the focus on pose graphs originally grounded in sparse methods and move towards a more map-centric approach that more elegantly fits the modelpredictive characteristics of a typical dense frontend</p><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><p>This section will describe how to carry out ElasticFusion on ubuntu 16.04 with a Kinect sensor.<br>The major source code is from: <a href="https://github.com/mp3guy/ElasticFusion" target="_blank" rel="noopener">https://github.com/mp3guy/ElasticFusion</a>. There is a file named build.sh, which can automatically install all kinds of dependences, but I am going to refer to this file and manually install all the dependences step by step as follow:<br>1) Installing CUDA 8.0<br>2) Installing all dependences<br>3) Installing Pangolin<br>4) Installing OpenNI24.<br>  From the official website to download the ZIP file and finding the install.sh and then executing $sudo ./install.sh Note: executing “cat OpenNIDevEnvironment&gt;&gt; ~/.bashrc” after installing OpenNI2 so that the directory of Include and Redist can be added in environment variables.</p><p>5) Build ElasticFusion</p><p>So far, we have already installed all the dependences that authors mention. However, what the tricky things are OPENNI2 used in this project does not support the Kinect V1. Therefore, after installation, executing “./ElasticFusion -l dyson_lab.klg”, where “dyson_lab.klg” is a provided package from authors, there is an error saying that: Creating live capture… failed! DeviceOpen using default: no devices found.</p><p>So, what I did is to download the third-party library named libFreenect from <a href="https://github.com/OpenKinect/libfreenect" target="_blank" rel="noopener">https://github.com/OpenKinect/libfreenect</a>. build this file and executing “cmake .. -DBUILD_OPENNI2_DRIVER=ON”. After that, copy the .so files which are all in the “libfreenect/lib/OpenNI2-FreenectDriver” to “OpenNI2/Bin/x64-Release/OpenNI2/Drivers”. You can executing “./NiViewer” to check if successfully install the driver.” But right now, do not forget to manually tell Cmake where OPENNI2. Therefore, open the CmakeList.txt file in src of GUI of ElasticFusion. Adding “set(OPENNI2_INCLUDE_DIR “ home/turtlebot/OpenNI2/Include”)” into this file and make again.<br>Currently, we have already installed all the dependences environments for ElasticFusion. The next step is to excute “. ElasticFusion -l dyson_lab.klg” and then you can see the visual window for mapping and constructing. You can provide a .klg log file with the -l parameter. You can capture .klg format logs using Logger1 from <a href="https://github.com/mp3guy/Logger1" target="_blank" rel="noopener">https://github.com/mp3guy/Logger1</a>.</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><div align="center"><img src="/2019/07/13/ElasticFusion/result.jpg"></div><br>As can been seen in the left most picture, the mapping is continuous, and the new mapping is aligned with the current one, which shown in different color. The middle figure shown in Figure 3 is doing the local loop closure that ensuring local surface consistency throughout the map the system closes many small loops with the existing map as those areas are revisited. However, while doing the global loop closure, this design does not perform well shown in the right most picture of the Figure 3. The objects lose in the mapping and is mixed into others plane.<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>1) This work is based on RGB-D dense three-dimensional reconstruction with mesh model commonly using fuse point cloud. ElasticFusion is one of the few that is represented by surfel model.<br>2) Traditional SLAM algorithm generally improves the accuracy of trajectory estimation or reconstruction by continuously optimizing camera trajectory or feature points. This paper adopts the method of continuously optimizing reconstructed map to improve the accuracy of reconstruction and pose estimation.<br>3) The optimization takes the form of deformation graph.<br>4) Reposition algorithm is integrated (re-calculate the camera pose when the camera is lost).<br>5) The algorithm updates, merges, displays and projects the point cloud using OpenGL.<br>6) The algorithm integrates RGB images by colour consistency constraint and point cloud by ICP algorithm to estimate the pose.<br>7) It is suitable for the reconstruction of room-size scenarios. The code is not optimized and cannot be applied when the larger scene is reconstructed, which means that this approach needs to address the problem of map scalability beyond whole rooms and put emphasis on the problem of dense globally consistent SLAM as t→∞.</p><p>More ElasticFusion or details of this project, please look through the project report.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] ElasticFusion: Real-Time Dense SLAM and Light Source Estimation, T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison and S. Leutenegger, IJRR ‘16<br>[2] ElasticFusion: Dense SLAM Without A Pose Graph, T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker and A. J. Davison, RSS ‘15<br>[3] Xingyin-Fu, <a href="https://blog.csdn.net/fuxingyin/article/details/51433793" target="_blank" rel="noopener">https://blog.csdn.net/fuxingyin/article/details/51433793</a><br>[4] mp3guy, <a href="https://github.com/mp3guy/ElasticFusion" target="_blank" rel="noopener">https://github.com/mp3guy/ElasticFusion</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;Demonstrating a novel method to real-time dense visual SLAM, this paper summaries that the system is able to capture comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using and RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. The method they used is applying local model-to-model surface loop closure optimisations as often as possible to stay closure to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency.&lt;/p&gt;
    
    </summary>
    
      <category term="Computer Vision" scheme="http://yoursite.com/categories/Computer-Vision/"/>
    
    
      <category term="Computer Vision" scheme="http://yoursite.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Shallow-Neural-Network</title>
    <link href="http://yoursite.com/2019/07/13/Shallow-Neural-Network/"/>
    <id>http://yoursite.com/2019/07/13/Shallow-Neural-Network/</id>
    <published>2019-07-13T07:31:54.000Z</published>
    <updated>2019-07-19T14:53:32.620Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This blog will introduce the simple neural networks, showing the neural network represntation, computing a neural network’s output, and indicating what is activation function and why we need a non-linear activation function. Besides that, the backpropagation will be introduced as well.</p><a id="more"></a><ul><li>Neural network representation</li><li>Computing the output of the neural network</li><li>Activation function</li><li>Backpropagation</li></ul><h2 id="Neural-networks-representation"><a href="#Neural-networks-representation" class="headerlink" title="Neural networks representation"></a>Neural networks representation</h2><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/representation.jpg"></div><br><p>Simply say that we have input $x_1, x_2, x_3$ and have one hidden layer with four neural units and finally we have one output unit in above figure showing a simple neural network architecture.</p><h2 id="Computing-the-output-of-the-neural-network-Feedforward"><a href="#Computing-the-output-of-the-neural-network-Feedforward" class="headerlink" title="Computing the output of the neural network (Feedforward)"></a>Computing the output of the neural network (Feedforward)</h2><p>In order to figure out the output of the neural network, let’s make some denotation.<br>Input: $x_1, x_2, x_3$. weight: $[w_1^{[1]},w_2^{[1]},w_3^{[1]},…,w_3^{[4]}$. Therefore we have:<br>$$<br>z=w^Tx+b<br>$$where term “b” is bias term. For example,we can obtain: $z_1^{[1]}=w_1^{[1]T}x+b$. And so on and so forth.<br>$$<br>z_2^{[1]}=w_2^{[1]T}x+b^{[1]}\\<br>z_3^{[1]}=w_3^{[1]T}x+b^{[1]}\\<br>z_4^{[1]}=w_4^{[1]T}x+b^{[1]}<br>$$<br>and then pass through the non-linear function “sigmoid function”:<br>$$a_1^{[1]}=\sigma(z_1^{[1]})\\<br>a_2^{[1]}=\sigma(z_2^{[1]})\\<br>a_3^{[1]}=\sigma(z_3^{[1]})\\<br>a_4^{[1]}=\sigma(z_4^{[1]})<br>$$<br>and then multiply the weight agian and pass through the sigmoid function:<br>$$<br>z_1^{[2]}=w_1^{[2]T}a^{[1]}+b^{[2]}\\<br>a_1^{[2]}=\sigma(z_1^{[2]})<br>$$</p><p>the general formula is:<br>$$<br>z^{[l]}=w^{[l]T}a^{[l-1]}+b^{[l]}\\<br>a^{[l]}=\sigma(z^{[l]})<br>$$<br>where $a^{[0]}$ is the input $x_1, x_2, x_3$</p><p>Therefore, $ \hat{y}= \ a^{[3]} $ in this case. And we can also directly can figure out that the dimension of weights, like:<br>$$<br>w^{[1]} : (4,3)\\<br>w^{[2]} : (4,4)\\<br>w^{[3]} : (1,4)\\<br>$$<br>the general format is like: $w^{[l]}$: <strong>(the numer of units of current layer, the numer of units of previous layer).</strong> This is useful to double check that the dimension is same no matter in feedforward or backward propagation.</p><h2 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h2><p>What we use above is so-called activation functions. If there is no activation function in neural network, the output of the model is only the linear combination with input features x. <strong>Therefore, we can not use any linear activation function among hidden layers, but can use the linear activation in the output layer, which is the special case.</strong><br>There are severl common used activation function, like “sigmoid”, “tanh”, “Relu”, and “leaky Relu”.</p><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/activation.jpg"></div><br><ul><li>sigmoid:<br>$$<br>a=\sigma(z)=\frac{1}{1+e^{-z}}\\<br>\frac{d}{dz}\sigma(z)=\sigma(z)(1-\sigma(z))<br>$$</li><li>tanh:<br>$$<br>a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\<br>\frac{d}{dz}tanh(z)=1-(tanh(z))^2<br>$$</li><li>Relu:<br>$$<br>a=max(0,z)$$</li></ul><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/Relu.jpg"></div><br><ul><li>leaky Relu<br>$$<br>a=max(0.01z,z)$$</li></ul><div align="center"><img src="/2019/07/13/Shallow-Neural-Network/Leaky Relu.jpg"></div><br><p>The common disadvantage of sigmoid and tanh is that when the case where $z$ is too large or too small happens, the derivative of the function will be enormously small, even approximate to zero, which will decrease the speed of gradient descent. Alternatively, the  activation function “Relu” is also a popular choice in machine learning. The advantage of “Relu” is that the derivative is zero when the z is negative number. As for “Leaky Relu”, when z is negative number, the magnitude of this function is not equal to zero, rather slightly incline.<br>The experience to select the activation function:</p><ul><li>sigmoid: output is the binary classification</li><li>tanh: almost suitable for most situation</li><li>Relu: the most common defaut activation function</li></ul><h2 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h2><p>Before doing the backpropagation, we define the coss function of this model. Here we use the cost function as follow:<br>$$<br>J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y_{i}\log\left(a^{[3] [i] }\right) + (1-y_{i})\log\left(1- a^{[3] [i]}\right) \large{)}<br>$$<br>where $y_i$ is the label of the $i_{th}$ sample and $a^{[3] [i]}$ denotes that the model ouput of the $i_{th}$ sample.</p><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>So far, we have already known what the feedforward, activation function and cost function are. So, let’s discuss about the backpropagation. For simplicity, we just consider only one example in this case, but for the whole samples, we can vectorize them and then using same principle.</p><p>Now for the derivativeof the $a^{[3]}$, we have:<br>$$<br>\frac{dJ}{da^{[3]}}=[-yloga^{[3]}-(1-y)log(1-a^{[3]})]^{‘}_{a^{[3]}}\\=-y\frac{1}{a^{[3]}}+(1-y)\frac{1}{1-a^{[3]}}<br>$$<br>And for the derivativeof the $z_1^{[3]}$, we have:<br>$$<br>\frac{dJ}{dz_1^{[3]}}=\frac{dJ}{da^{[3]}}\bullet\frac{da^{[3]}}{dz_1^{[3]}}\\<br>=(y\frac{1}{a^{[3]}}+(1-y)\frac{1}{1-a^{[3]}})\bullet\sigma(z_1^{[3]})(1-\sigma(z_1^{[3]}))\\=(y\frac{1}{a^{[3]}}+(1-y)\frac{1}{1-a^{[3]}})\bullet a^{[3]}(1-a^{[3]})\\=-y(1-a^{[3]})+(1-y)a^{[3]}<br>$$<br><font size="6">$\Rightarrow$ </font><br>$$<br>  \frac{dJ}{dz_1^{[3]}}=a^{[3]}-y<br>$$</p><p>where we use “sigmoid” function here. Similarly, we can use chain rule to derive:<br>$$<br>\frac{dJ}{dw_{11}^{[3]}}=\frac{dJ}{da^{[3]}}\frac{da^{[3]}}{dz_1^{[3]}}\frac{dz_1^{[3]}}{dw_{11}^{[3]}}\\=\frac{dJ}{da^{[3]}}\frac{da^{[3]}}{dz_1^{[3]}}a_1^{[2]}\\=\frac{dJ}{dz_1^{[3]}}a_1^{[2]}<br>$$<br>As for bias term $b$, we have:<br>$$<br>\frac{dJ}{db_1^{[3]}}=\frac{dJ}{da^{[3]}}\frac{da^{[3]}}{dz_1^{[3]}}\frac{dz_1^{[3]}}{db_{1}^{[3]}}\\=\frac{dJ}{dz_1^{[3]}}<br>$$<br>where $\frac{dz_1^{[3]}}{db_{1}^{[3]}} = 1$</p><br>So far, we can have general form as following:<p>$$<br>\frac{dJ}{dw^{[l]}}=\frac{dJ}{dz^{[l]}}\bullet a^{[l-1]}<br>$$<br>$$<br>\frac{dJ}{db^{[l]}}=\frac{dJ}{dz^{[l]}}<br>$$</p><p>so that we can update the parameters by using:<br>$$<br>w^{[l]}=w^{[l]}-\alpha \frac{dJ}{dw^{[l]}}<br>$$<br>$$<br>b^{[l]}=b^{[l]}-\alpha \frac{dJ}{db^{[l]}}<br>$$</p><p>This is the backpropagation process of a single sample.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;This blog will introduce the simple neural networks, showing the neural network represntation, computing a neural network’s output, and indicating what is activation function and why we need a non-linear activation function. Besides that, the backpropagation will be introduced as well.&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Change-detection-using-GMM</title>
    <link href="http://yoursite.com/2019/07/12/Change-detection-using-GMM/"/>
    <id>http://yoursite.com/2019/07/12/Change-detection-using-GMM/</id>
    <published>2019-07-12T12:23:13.000Z</published>
    <updated>2019-07-19T14:53:37.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Change detection is one of the most commonly used methods for detecting moving objects in a computer vision system. It detects moving objects in an image sequence by comparing the background model with the current frame. Traditional moving target detection algorithms have problems such as dynamic background changes caused by illumination changes, noise and shadow sensitivity.</p><a id="more"></a><p>In this project, a method based on Gaussian Mixture Model (GMM) is introduced for the challenge of object detection, which modeling each pixel of each frame as a mixture of Gaussians and using an on-line approximation to update the model. Based on whether the Gaussian distribution that represents it most effectively is considered part of the background model, each pixel is classified. This report is targeted to utilizes the python to implement the GMM algorithm to segment background for several videos<br><br><br>Repo: <a href="https://github.com/TianchaoHuo/Change-Detection-Using-GMM" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Change-Detection-Using-GMM</a></p><h2 id="Objective"><a href="#Objective" class="headerlink" title="Objective:"></a>Objective:</h2><p>1.Detecting the moving object in videos recorded by a stationary camera<br>2.Segmentation between foreground and background<br>3.Adaptive in working conditions<br>4.Not costly develop</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>PETS 2009 (<a href="ftp://pets.rdg.ac.uk" target="_blank" rel="noopener">ftp://pets.rdg.ac.uk</a>)</p><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><p>This implementataion is based on the python 3 and open-cv package.<br>Hence, you need to install the related implementation environment.</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=58925991&cid=102725017&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>Pros</li></ul><ol><li>Be adaptive to different scenes</li><li>Be capable to segment videos</li><li>Good trade-off between quality and execution time</li></ol><ul><li>Cons</li></ul><ol><li>Arbitrarily set the parameters</li><li>Assume each pixel is independent of its neighbors.</li></ol><p>More hidden principle of GMM or details of this project, please look through the project report.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] C. Stauffer and W. E. L. Grimson, “Adaptive background mixture models for real-time tracking,” in 1999, . DOI: 10.1109/CVPR.1999.784637.<br>[2] H. Shih-Shinh, <a href="https://www.youtube.com/watch?v=g_ve2txPkSc" target="_blank" rel="noopener">https://www.youtube.com/watch?v=g_ve2txPkSc</a>, 2018.<br>[3] Swapnil Das, <a href="https://github.com/swapnil96" target="_blank" rel="noopener">https://github.com/swapnil96</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;Change detection is one of the most commonly used methods for detecting moving objects in a computer vision system. It detects moving objects in an image sequence by comparing the background model with the current frame. Traditional moving target detection algorithms have problems such as dynamic background changes caused by illumination changes, noise and shadow sensitivity.&lt;/p&gt;
    
    </summary>
    
      <category term="Computer Vision" scheme="http://yoursite.com/categories/Computer-Vision/"/>
    
    
      <category term="Computer Vision" scheme="http://yoursite.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Friut Detection, Tracking and Counting</title>
    <link href="http://yoursite.com/2019/07/11/FriutDetectionTrackingandCounting/"/>
    <id>http://yoursite.com/2019/07/11/FriutDetectionTrackingandCounting/</id>
    <published>2019-07-11T13:29:45.000Z</published>
    <updated>2019-07-19T14:53:48.592Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><hr><p>This project is designed to do fruit detection, traking and counting. Videos are already provided.</p><p>Co-Author: Chaohuo Wu, Xiaochen Liu, Do Kyoun Lee</p><p>Repo: <a href="https://github.com/TianchaoHuo/Fruit-Detection" target="_blank" rel="noopener">https://github.com/TianchaoHuo/Fruit-Detection</a></p><a id="more"></a><h1 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h1><hr><p> 1 . Read in a video clip, and display the video frame by frame,<br> 2 . Detect all lemons and bananas in each frame, segment the corresponding pixel regions, count the number of pixels, draw a bounding box on these detected fruits.<br> 3 . Track the detected oranges and bananas in the entire video sequence, until they disappear.<br> 4 . Count the number of fruits of each type, and update the counting result in real-time.<br> 5 . At the end of each video clip, report the success rate of your program.</p><h1 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h1><hr><p>Open the video named ‘Test.mp4’. This is the one of the testing video.  </p><h1 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h1><hr><p>1 . DownLoad the project material from google driver. The link is: <a href="https://drive.google.com/open?id=1yQXSwYu-GljAxXZLB0YlPoV0lx7KVMPh" target="_blank" rel="noopener">https://drive.google.com/open?id=1yQXSwYu-GljAxXZLB0YlPoV0lx7KVMPh</a><br>2 . Open MATLAB software.<br>3 . Decompress the ‘project material.zip’ and copy the ‘fruitDetectionTrackingCounting.m’ into the folder.<br>4 . Change the path to the folder.<br>4 . Load the ‘pal2.mat’ which contains the Faster RCNN detector.<br>4 . Input ‘global pal2’ in command window.<br>5 . Open and run the ‘FruitDetectionTrackingCounting’ script.</p><ul><li>important: Before running the script, please make sure you have already input ‘global pal2’ in command window, otherwise an exception will occur<br>‘function ‘detect’ for uint 8 variables are not defined.</li><li>Note: Since maximum size for submission files on wattle is 200MB, some large but important materials need to be download from goole driver.</li></ul><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h5 id="Visually-Demo"><a href="#Visually-Demo" class="headerlink" title="Visually Demo"></a>Visually Demo</h5><iframe width="700" height="500" src="//player.bilibili.com/player.html?aid=58801015&cid=102520996&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><br><br></p><h5 id="Accuracy-of-this-model"><a href="#Accuracy-of-this-model" class="headerlink" title="Accuracy of this model"></a>Accuracy of this model</h5><div align="center"><img src="/2019/07/11/FriutDetectionTrackingandCounting/table.png"></div><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><hr><p>We used Transfer Learning, Alexnet network, Faster-RCNN, Kalman filtert, and others techology to implement this project and have tested all provided videos. The performance is shown in the report ‘Fruit Detection, Tracking and Counting_Team 4.pdf’.</p><h1 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h1><p> [1]Motion-Based-Multiple-object-tracking, <a href="https://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html" target="_blank" rel="noopener">https://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html</a><br> [2] A. Krizhevsky, I. Sutskever and G. Hinton, “ImageNet classification with deep convolutional neural networks,” Communications of the ACM, vol.60, (6), pp. 84-90, 2017. . DOI: 10.1145/3065386.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;This project is designed to do fruit detection, traking and counting. Videos are already provided.&lt;/p&gt;
&lt;p&gt;Co-Author: Chaohuo Wu, Xiaochen Liu, Do Kyoun Lee&lt;/p&gt;
&lt;p&gt;Repo: &lt;a href=&quot;https://github.com/TianchaoHuo/Fruit-Detection&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/TianchaoHuo/Fruit-Detection&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Computer Vision" scheme="http://yoursite.com/categories/Computer-Vision/"/>
    
    
      <category term="Computer Vision" scheme="http://yoursite.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="http://yoursite.com/2019/07/11/Linear-Regression/"/>
    <id>http://yoursite.com/2019/07/11/Linear-Regression/</id>
    <published>2019-07-11T11:59:55.577Z</published>
    <updated>2019-07-19T14:53:54.874Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Basically we can generally classify the linear regression as two different types.</p><ul><li>Linear regression with one variable</li><li>Linear regression with multiple variables</li></ul><a id="more"></a><p>1.For the first type of Linear regression, it only has one factor and a coresponding label. As shown in the forllowing figure[2], the size of the house is the variable and the price is so-called label. What we wanna do is to find a line or curve to fit the data points and then estimate the price with given a new data point.</p><div align="center"><img src="/2019/07/11/Linear-Regression/houseprice.jpg"></div><br><p>2.Similarly , for the second type of Linear regression, it has mutiple variables to determine the housing price, such as the size, transportaion, schools, etc. Instead of using the single variable described above, a feature matrix is utilized in Linear regression.</p><p>So, this is the basic concept of liear regression with one variable.</p><h2 id="Mathematical-Principle"><a href="#Mathematical-Principle" class="headerlink" title="Mathematical Principle"></a>Mathematical Principle</h2><p>This section is going to elaborate the hidden principle of Linear regression. At the begining, we need to make some denotation for effective description. The objective of Linear regression is to find a line or curve that fits data and then to estimate with given a new input.</p><h3 id="Modelling"><a href="#Modelling" class="headerlink" title="Modelling"></a>Modelling</h3><p>Simply denoting the input as :</p><p>$$x = [x_1,x_2,x_3,…,x_m]^T$$<br>note: $x_m$ here represnts a feature vector of the $m_{th}$ sample and the label is denoting as:<br>$$y = [y_1,y_2,y_3,…,y_m]^T$$</p><p>what we would like to do is to find a line :</p><p>$$h_{w}(x) = w_0 + w_1 x_1 + w_2 x_2 + …. + w_m x_m$$</p><p>Therefore, we need to find the parameters $w$ to obtain a good line or curve and then do estimation.<br>The beautiful expectation is that the estimated value is close to the truth value as possible as it can. In other words, the target is to minimize the distance between predicted value and truth value. Hence, we introude a function called “Cost function”, which is also named “Square Error function”.:</p><p>$$ J(w)=\frac{1}{2m}\sum_{i=1}^m (h_{w}(x_{(i)}) - y_{(i)})^2$$</p><p>$h_{w}(x_{(i)})$  denotes the predition of this model. As described before, the objective is to minimize the error by figuring out the parameters $w$.</p><h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p>Obviously that the cost function is a convex function, so that we just take a derivative with respect to the $w$ and then let its derivative equal to zero to obtain the weight at the local minimum point. Next, we need to update the weight, $w$ by traversing the whole data set from $x_1$ to $x_m$ using forllowing equation:<br>$$w_i = w_i - \alpha \frac{d}{dw}J(w)$$<br>$\alpha$ is represented as learning rate.<br>This method is named as gradient descent algorithm, which will continuously update the weight and then the cost function will gradually step to the minimum point.</p><h2 id="Statistical-description"><a href="#Statistical-description" class="headerlink" title="Statistical description"></a>Statistical description</h2><h3 id="Minimum-Square-Error"><a href="#Minimum-Square-Error" class="headerlink" title="Minimum Square  Error"></a>Minimum Square  Error</h3><p>continuously using above denotation, we introude the Minimum Square Error(MSE) algorithm. We have a cost function and to simplfy the computation, we omit the constant scaling varaibale  $\frac{1}{2m}$ and replace $h_w(x_{(i)})$ with $w^Tx_i$<br>$$ J(w)=\sum_{i=1}^m ||t_i - y_i||^2=\sum_{i=1}^m (w^Tx_i- y_i)^2$$<br>where $t_i$ is the output of the mdoel.<br>Therefore, we have</p><div align="center"><img src="/2019/07/11/Linear-Regression/formula.jpg"></div><p>$$X^TXW=X^TY  $$<br>Finally, we obtatin:<br>$$W^* =(X^TX)^{-1}X^TY$$<br>Or we can directly compute the weight, $w$, in this way:<br>$$J(w)=||t_i-y_i||^2$$<br>Take the Gradient<br>$$\frac{d}{dw}J(w)=2(XW-Y)^TX$$<br>solve for stationary point<br>$$X^TXW=X^TY$$</p><p><br><br></p><h3 id="Maximum-likelihood"><a href="#Maximum-likelihood" class="headerlink" title="Maximum likelihood"></a>Maximum likelihood</h3><p>From the Probability persepctive, we supoose that $ \epsilon \thicksim N(0, \sigma^2)$，just ignoring bias term $b$ here for simpicity. Hence, $ t=y(x,w)+\epsilon=w^Tx+\epsilon$. Therefore, what we wanna do is to maximise the prediction probability, $p(t|x,w)$, given data and weight. It is apparent that:<br>$$p(t|x,w)\thicksim N(w^T,\sigma^2)$$<br>$$p(t|x,w)=\frac{1}{\sqrt{2\pi}\sigma} exp[-\frac{(t-w^Tx)^2}{2\sigma^2}]<br>$$<br>We definitely aim to find the largest probability, which means that we are looking for a method that maximum likelihood. There are two persepctives to solve this problem.</p><ul><li>Frequentis Aprroach</li><li>Bayesian Approach</li></ul><ol><li>Frequentis Approach<br>Maximum likelihood Estimate (MLE):<br>$$J(w)=logp(t|x,w)=log\prod_{i=1}^mp(t_i|x_i,w)$$<br>$$=\sum_{i=1}^mlogp(t_i|x_i,w) =\sum_{i=1}^m\left(log\frac{1}{\sqrt{2\pi}\sigma}+logexp[-\frac{(t_i-w^Tx_i)^2}{2\sigma^2}]\right)$$<br>$$=\sum_{i=1}^m\left(log\frac{1}{\sqrt{2\pi}\sigma}+\frac{1}{2\sigma^2}(t_i-w^Tx_i)^2\right)$$<br>$\Rightarrow$<br>$$\hat{w}=argmax_wJ(w)=argmax_w\sum_{i=1}^m(-\frac{1}{2\sigma^2}(t_i-w^Tx_i)^2) = argmin_w\sum_{i=1}^m(t_i-w^Tx_i)^2$$<br>so far, the above equation is the same as shown in minimum cost function, therefore just taking gradient and then equalling to zero, then we can obtain<br>$$\hat{w}=(X^TX)^{-1}X^TY$$</li></ol><ol start="2"><li>Bayesian Approach (MAP)<br>Similarly at start, we supoose that $w\thicksim N(0,\beta^2)$, so we have:<br>$$p(w)=\frac{1}{\sqrt{2\pi}\beta^2}exp[-\frac{||w||^2}{2\beta^2}]<br>$$<br>and we aslo have:<br>$$p(t|x,w)=\frac{1}{({2\pi})^{\frac{1}{2}}\sigma}exp [-\frac{(t-w^Tx)^2}{2\sigma^2} ] $$<br>Therefore, according to the Bayesian rules:<br>$$p(w|t)=\frac{p(t|w)p(w)}{p(t)}$$<br>The term, $p(w|t)$, is posterior,  $p(t|w)$ is likelihood, $p(w)$ is prior.<br>we wanna maximum the likelihood and the posterior is proportional to  likelihood. So we have a method : Maximum a Posterior Estimation (MAP):<br>$$\hat{w}=argmax_wp(w|t) =argmax_wp(t|w)\bullet p(w)$$<br>$$=argmax_wlog(p(t|w)\bullet p(w))$$<br>$$=argmax_w\left(log(\frac{1}{\sqrt{2\pi}\sigma}\bullet \frac{1}{\sqrt{2\pi}\beta})+log exp [-\frac{(t-w^Tx)^2}{2\sigma^2}-\frac{||w||^2}{2\beta^2} ] \right)$$<br>so, deleting the first term, constant variables, we have:<br>$$argmin_wlog(\frac{(t-w^Tx)^2}{2\sigma^2}+\frac{||w||^2}{2\beta^2})$$<br>$$  =argmin_w\sum_{i=1}^{m}\left((t_i-w^Tx_i)^2+\frac{\sigma^2}{\beta^2}||w||^2\right)$$<br>Therefore, it finally turns into the minimum cost function and using above method, we can have the same result:<br>$$\hat{w}=(X^TX)^{-1}X^TY$$</li></ol><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>However, you might be confused that what the second term is in above equation. Here we need to introude a term “regularization”. When we fit the data using a line or curve, probably it might overfit the data, let say what that mean[2]:</p><div align="center"><img src="/2019/07/11/Linear-Regression/regularization.jpg"></div><br><p>The above figure has three types of fitting, coresponding “underfitting”, “just right”, and “overfitting” respectively. The left one can not represent the data well while the right one fit the data perfectly, which the ability of prediction is relatively worse. Therefore, to address this problem, we add a regularized term to avoid overfitting. It works well when we have a lot of feature, each of which contributes a bit to predicting $t$ because it remains all the features but just reduces the magnitude of the parameters $w$. This is the fundamental idea of regularization. Therefore, changed cost function can be:<br>$$<br>J(w)=\frac{1}{2m}[\sum_{i=1}^m(t_i-w^Tx_i)^2+\lambda\sum_{i=1}^mw_i^2]<br>$$<br>where $\lambda$ is so-called regularization parameter. Apparently as increasing the $\lambda$, all the parameters, $w$, decrease. But if $\lambda$ is too large, the weight $w$ will be approximately to 0, then we will have a line paralleling to x axis. So, we need to have a reasonable magnitude of $\lambda$.<br>We also can obviously see that the $\lambda$ here is equal to $\frac{\sigma^2}{\beta^2}$.<br>So far, we have already known the cost function with regularization term so that we can minimum the cost function. Ignoring the constant varaibale for simplicity:<br>$$<br>J(w)=\sum_{i=1}^m||t_i-w^Tx_i||^2+\lambda w^Tw $$<br>$$<br> =w^TX^TXw-2w^TX^TY+Y^TY+\lambda w^Tw<br>$$<br>$$<br>=w^T(X^TX+\lambda)w-2w^TX^TY+Y^TY<br>$$<br>$\Rightarrow$<br>$$<br>\hat{w}=argmin_wJ(w)\Rightarrow\frac{d}{dw}J(w)=2(X^TX+\lambda)w-2X^TY=0$$<br>$$<br> \therefore \hat{w}=(X^TX+\lambda I)^{-1}X^TY<br>$$</p><p><br><br><br></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Before wraping up the Linear regression section, let’s draw a figure to show the relationship among the LSE, MLE and MAP discussed above.</p><div align="center"><img src="/2019/07/11/Linear-Regression/relationship.jpg"></div><br><p>The next step is to implement these algorithm using python.</p><font size="4" face="Times New Roman"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.path = <span class="string">'ex1data1.txt'</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(self, X, y, theta)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(np.power((X*theta.T)-y),<span class="number">2</span>)/(<span class="number">2</span>*len(X))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(self, X, y theta, learningrate, epoch)</span>:</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>]) <span class="comment"># ravel flatten function</span></span><br><span class="line">    cost = np.zeros(epoch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">      error = (X * theta.T - y)</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error,X[:,j])</span><br><span class="line">        temp[<span class="number">0</span>,j] = theta[<span class="number">0</span>,j] - ((alpha/len(X))*np.sum(term))</span><br><span class="line"></span><br><span class="line">      theta = temp</span><br><span class="line">      cost[i] = self.computeCost(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T @ X) @ X.T @ y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">  Admin = LinearRegression()</span><br><span class="line">  data = pd.read_csv(Admin.path, header=<span class="literal">None</span>, names=&#123;<span class="string">'Population'</span>, <span class="string">'Profit'</span>&#125;)</span><br><span class="line">  data.plot(kind=<span class="string">'scatter'</span>, x=<span class="string">'Population'</span>, y=<span class="string">'Profit'</span>, figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">  plt.show()</span><br><span class="line">  <span class="comment"># adding a column of ones will achieve vectorization</span></span><br><span class="line">  data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line">  <span class="comment"># set X(training data) and y(target variable)</span></span><br><span class="line">  cols = data.shape[<span class="number">1</span>]</span><br><span class="line">  X = data.iloc[:, <span class="number">0</span>:cols<span class="number">-1</span>]</span><br><span class="line">  y = data.iloc[:, cols<span class="number">-1</span>:cols]</span><br><span class="line">  <span class="comment"># convert from data framse to numpy matrices</span></span><br><span class="line">  X = np.matrix(X.values)</span><br><span class="line">  y = np.matrix(y.values)</span><br><span class="line">  theta = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line">  <span class="comment"># set parameters and do calculation</span></span><br><span class="line">  alpha = <span class="number">0.01</span></span><br><span class="line">  epoch = <span class="number">1000</span></span><br><span class="line">  g,cost = Admin.gardientDescent(X, y, theta, alpha, epoch)</span><br><span class="line">  temp = Admin.computeCost(X, y, g)</span><br><span class="line">  print(g)</span><br><span class="line">  <span class="comment"># plotting the result</span></span><br><span class="line">  x = np.linspace(data.Population.min(), data.Population.max(), <span class="number">100</span>)</span><br><span class="line">  print(x.shape)</span><br><span class="line">  f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line">  print(f.shape)</span><br><span class="line">  fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">  ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">  ax.scatter(data.Population, data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">  ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">  ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">  ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">  ax.set_title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">  ax.plot(np.arange(epoch), cost, <span class="string">'r'</span>)</span><br><span class="line">  ax.set_xlabel(<span class="string">'Iterations'</span>)</span><br><span class="line">  ax.set_ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">  ax.set_title(<span class="string">'Error vs. Training Epoch'</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  <span class="comment">#  normalize equation</span></span><br><span class="line">  <span class="comment"># diretly using LSE equation insteady of gradient descent.</span></span><br><span class="line">  final_theta2 = Admin.normalEqn(X, y)</span><br><span class="line">  print(final_theta2)</span><br></pre></td></tr></table></figure></font><br><br><h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><p>[1] 黄海广, <a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">https://github.com/fengdu78/deeplearning_ai_books</a><br>[2] Andrew Ng, <a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a><br>[3] shuhuai007 <a href="https://github.com/shuhuai007/Machine-Learning-Session" target="_blank" rel="noopener">https://github.com/shuhuai007/Machine-Learning-Session</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;Basically we can generally classify the linear regression as two different types.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression with one variable&lt;/li&gt;
&lt;li&gt;Linear regression with multiple variables&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>My First Post</title>
    <link href="http://yoursite.com/2019/07/10/my-first-post/"/>
    <id>http://yoursite.com/2019/07/10/my-first-post/</id>
    <published>2019-07-09T15:51:40.000Z</published>
    <updated>2019-07-13T16:46:37.010Z</updated>
    
    <content type="html"><![CDATA[<div align="center"><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=1868008226,1930371026&fm=26&gp=0.jpg"><a id="more"></a><p>END</p></div>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=1868008226,1930371026&amp;fm=26&amp;gp=0.jpg&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/07/10/hello-world/"/>
    <id>http://yoursite.com/2019/07/10/hello-world/</id>
    <published>2019-07-09T15:47:21.479Z</published>
    <updated>2019-07-09T15:47:21.479Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
